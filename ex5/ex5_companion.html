
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,IE=9,chrome=1"><meta name="generator" content="MATLAB"><title>MATLAB Companion Script for Machine Learning ex5 (Optional)</title><style type="text/css">.rtcContent { padding: 30px; } .S0 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 28.8px; min-height: 0px; white-space: pre-wrap; color: rgb(213, 80, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 24px; font-weight: 400; text-align: left;  }
.S1 { margin: 20px 10px 5px 4px; padding: 0px; line-height: 20px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 20px; font-weight: 700; text-align: left;  }
.S2 { margin: 2px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: left;  }
.S3 { margin: 10px 0px 20px; padding-left: 0px; font-family: Helvetica, Arial, sans-serif; font-size: 14px;  }
.S4 { margin-left: 56px; line-height: 21px; min-height: 0px; text-align: left; white-space: pre-wrap;  }
.S5 { margin-bottom: 20px; padding-bottom: 4px;  }
.S6 { margin: 0px; padding: 10px 0px 10px 5px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 700; text-align: start;  }
.S7 { margin: -1px 0px 0px; padding: 10px 0px 10px 7px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: start;  }
.CodeBlock { background-color: #F7F7F7; margin: 10px 0 10px 0;}
.S8 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 1px solid rgb(233, 233, 233); border-bottom: 0px none rgb(0, 0, 0); border-radius: 4px 4px 0px 0px; padding: 6px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S9 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 0px none rgb(0, 0, 0); border-radius: 0px; padding: 0px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S10 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 0px 0px 4px 4px; padding: 0px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S11 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 20px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 20px; font-weight: 700; text-align: left;  }
.S12 { margin: 10px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: left;  }
.S13 { margin: 10px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 15px; font-weight: 700; text-align: left;  }
.S14 { border-left: 0px none rgb(0, 0, 0); border-right: 0px none rgb(0, 0, 0); border-top: 0px none rgb(0, 0, 0); border-bottom: 0px none rgb(0, 0, 0); border-radius: 0px; padding: 0px; line-height: 16px; min-height: 16px; white-space: pre; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 12px;  }
.S15 { margin: 15px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 17px; font-weight: 700; text-align: left;  }
.S16 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 17px; font-weight: 700; text-align: left;  }</style></head><body><div class = rtcContent><h1  class = 'S0' id = 'T_13370202' ><span>MATLAB Companion Script for </span><span style=' font-style: italic;'>Machine Learning</span><span> ex5 (Optional)</span></h1><h2  class = 'S1' id = 'H_5E50F655' ><span>Introduction</span></h2><div  class = 'S2'><span>Coursera's</span><span style=' font-style: italic;'> Machine Learning</span><span> was designed to provide you with a greater understanding of machine learning algorithms- what they are, how they work, and where to apply them. You are also shown techniques to improve their performance and to address common issues. As is mentioned in the course, there are many tools available that allow you to use machine learning algorithms </span><span style=' font-style: italic;'>without</span><span> having to implement them yourself. This Live Script was created by MathWorks to help </span><span style=' font-style: italic;'>Machine Learning</span><span> students explore the data analysis and machine learning tools available in MATLAB.</span></div><h2  class = 'S1' id = 'H_8D135D31' ><span>FAQ</span></h2><div  class = 'S2'><span style=' font-weight: bold;'>Who is this intended for?</span></div><ul  class = 'S3'><li  class = 'S4'><span>This script is intended for students using MATLAB Online who have completed ex5 and want to learn more about the corresponding machine learning tools in MATLAB.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>How do I use this script?</span></div><ul  class = 'S3'><li  class = 'S4'><span>In the sections that follow, read the information provided about the data analysis and machine learning tools in MATLAB, then run the code in each section and examine the results. You may also be presented with instructions for using a MATLAB machine learning app. This script should be located in the ex5 folder which should be set as your Current Folder in MATLAB Online.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>Can I use the tools in this companion script to complete the programming exercises?</span></div><ul  class = 'S3'><li  class = 'S4'><span>No. Most algorithm steps implemented in the programming exercises are handled automatically by MATLAB machine learning functions. Additionally, the final results will be similar, but not identical, to those in the programming exercises due to differences in implementation, parameter settings, and randomization.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>Where can I obtain help with this script or report issues?</span></div><ul  class = 'S3'><li  class = 'S4'><span>As this script is not part of the original course materials, please direct any questions, comments, or issues to the </span><span style=' font-style: italic;'>MATLAB Help</span><span> discussion forum.</span></li></ul><h1  class = 'S0' id = 'T_CBD47DB1' ><span>Regularized Linear Regression and Cross-Validation</span></h1><div  class = 'S2'><span>In this Live Script, we will use functions and apps from the </span><a href = "https://www.mathworks.com/products/statistics.html"><span>Statistics and Machine Learning Toolbox</span></a><span> to partition data into training and validation sets and determine optimal regularization strengths using cross-validation.</span></div><h2  class = 'S1' id = 'H_CD0E5564' ><span>Files needed for this script</span></h2><ul  class = 'S3'><li  class = 'S4'><span style=' font-family: monospace;'>ex5data1.mat</span><span> - Waterflow dataset</span></li></ul><div  class = 'S5'><div  class = 'S6'><span style=' font-weight: bold;'>Table of Contents</span></div><div  class = 'S7'><a href = "#T_13370202"><span>MATLAB Companion Script for Machine Learning ex5 (Optional)
</span></a><span>    </span><a href = "#H_5E50F655"><span>Introduction
</span></a><span>    </span><a href = "#H_8D135D31"><span>FAQ
</span></a><a href = "#T_CBD47DB1"><span>Regularized Linear Regression and Cross-Validation
</span></a><span>    </span><a href = "#H_CD0E5564"><span>Files needed for this script
</span></a><a href = "#T_06053CD9"><span>Data Partitioning and High Bias and High Variance Models
</span></a><span>    </span><a href = "#H_4FACA3B1"><span>Load the data
</span></a><span>    </span><a href = "#H_BF6F514E"><span>Partition the dataset
</span></a><span>    </span><a href = "#H_A19F3515"><span>Fit a high bias and high variance model to the training data
</span></a><a href = "#T_2376D504"><span>Regularized Linear Regression using fitrlinear
</span></a><span>    </span><a href = "#H_94B12883"><span>Load and normalize the data
</span></a><span>    </span><a href = "#H_799DBC89"><span>Partition the data
</span></a><span>    </span><a href = "#H_C63F5E20"><span>Fit linear regression models using multiple  values
</span></a><span>    </span><a href = "#H_1A0C2528"><span>Working with partitioned model variables
</span></a><span>    </span><a href = "#H_D0E143AD"><span>Evaluate model performance and select models using kfoldLoss and selectModels
</span></a><span>    </span><a href = "#H_93A4BC25"><span>Predict the response for all models and visualize the results
</span></a><a href = "#T_9F8A9BD8"><span>Hyperparameter Optimization
</span></a><span>    </span><a href = "#H_2B6854DA"><span>Specify the cross-validation method
</span></a><span>    </span><a href = "#H_A8A3614D"><span>Automatically select  using hyperparameter optimization
</span></a><a href = "#T_DB0B4D05"><span>K-Fold Cross-Validation
</span></a><span>    </span><a href = "#H_8402A8B0"><span>Introduction
</span></a><span>    </span><a href = "#H_A6D1734D"><span>Using K-fold cross-validation instead of hold-out validation
</span></a><span>            </span><a href = "#H_6B33E428"><span>Create a K-fold cvpartition variable for use with fitlm or fitglm:
</span></a><span>            </span><a href = "#H_C3CA5B8E"><span>Use K-fold cross-validation with fitrlinear or fitclinear
</span></a><span>            </span><a href = "#H_84F2EF93"><span>Use K-fold cross-validation and hyperparameter optimization with fitrlinear or fitclinear
</span></a><span>    </span><a href = "#H_682A76AA"><span>Use K-fold cross-validation with the Regression or Classification Learner Apps
</span></a><span>        </span><a href = "#H_FE75A9EF"><span>Open the app and select the variables
</span></a><span>        </span><a href = "#H_150C2B14"><span>Select and train the model
</span></a><span>        </span><a href = "#H_07954395"><span>Export and visualize the model</span></a></div></div><h1  class = 'S0' id = 'T_06053CD9' ><span>Data Partitioning and High Bias and High Variance Models</span></h1><div  class = 'S2'><span>Many functions used to train models in the Statistics and Machine Learning Toolbox are equipped to partition a dataset into training and cross-validation sets automatically. However, creating a fixed partition is often helpful for reproducibility of results, or when comparing the performance of several different algorithms. These partitions can also be useful when training models using functions for building and training simpler models like </span><span style=' font-family: monospace;'>fitlm</span><span> and </span><span style=' font-family: monospace;'>fitglm</span><span> which do not partition data automatically or provide options for validation. In this section we will use the </span><span style=' font-family: monospace;'>cvpartition</span><span> function to partition the water flow data from ex5.</span></div><h2  class = 'S1' id = 'H_4FACA3B1' ><span>Load the data</span></h2><div  class = 'S2'><span>Recall that the data contained in </span><span style=' font-family: monospace;'>ex5data1.mat</span><span> is divided into three sets:</span></div><ul  class = 'S3'><li  class = 'S4'><span>A </span><span style=' font-weight: bold;'>training</span><span> set to train the model</span></li><li  class = 'S4'><span>A </span><span style=' font-weight: bold;'>cross-validation</span><span> set to determine the regularization parameter</span></li><li  class = 'S4'><span>A </span><span style=' font-weight: bold;'>test</span><span> set to evaluate performance. </span></li></ul><div  class = 'S2'><span>In MATLAB, there is no need to store training and validation sets in separate variables. Run the code below to combine the training, test, and validation sets into a single </span><span style=' font-family: monospace;'>table</span><span> variable, </span><span style=' font-family: monospace;'>data</span><span>, and clear the original variables.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>load </span><span style="color: rgb(160, 32, 240);">ex5data1.mat</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>LevelChange = [X;Xtest;Xval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Outflow = [y;ytest;yval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data = table(LevelChange,Outflow);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>clearvars </span><span style="color: rgb(160, 32, 240);">-except data</span><span>;</span></span></div></div></div><h2  class = 'S11' id = 'H_BF6F514E' ><span>Partition the dataset</span></h2><div  class = 'S2'><span>Next we will use the </span><a href = "https://www.mathworks.com/help/stats/cvpartition.html"><span style=' font-family: monospace;'>cvpartition</span></a><span> function to partition the water flow data into a training set with 12 examples (for direct comparison with your results from ex5) and validation set with the remaining data. The result will be a </span><a href = "https://www.mathworks.com/help/stats/cvpartition-class.html"><span style=' font-family: monospace;'>cvpartition</span></a><span style=' font-style: italic;'> </span><span>variable that contains the partition information and the indicies of corresponding to the two sets. This variable can then be passed to MATLAB machine learning functions along with the original dataset for model training and validation. Alternatively, we can also extract a binary vector corresponding to the training and validation examples from the partition variable using the </span><span style=' font-family: monospace;'>training</span><span> and </span><span style=' font-family: monospace;'>test</span><span> functions, as demonstrated below, for use with functions like </span><span style=' font-family: monospace;'>fitlm</span><span> and </span><span style=' font-family: monospace;'>fitglm</span><span>. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>m = height(data); </span><span style="color: rgb(60, 118, 61);">% height() returns the number of rows in a table</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>c = cvpartition(m,</span><span style="color: rgb(160, 32, 240);">'HoldOut'</span><span>,m-12) </span><span style="color: rgb(60, 118, 61);">% Choose 12 training examples</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>tinds = training(c) </span><span style="color: rgb(60, 118, 61);">% Training indices</span></span></div></div></div><div  class = 'S12'><span style=' font-weight: bold;'>Note</span><span>: In the terminology of the Statistics and Machine Learning Toolbox, 'test' sets correspond to the 'validation' sets described in class. A 'test' set as described in the course - data not used in training OR validation - would still need to be stored in a separate </span><span style=' font-family: monospace;'>table</span><span> or matrix.</span></div><h2  class = 'S11' id = 'H_A19F3515' ><span>Fit a high bias and high variance model to the training data</span></h2><div  class = 'S2'><span>As in ex5, we will fit a linear model as an example of a high bias model, followed by an 8th degree polynomial model as a high variance model example. The code below uses </span><span style=' font-family: monospace;'>fitlm</span><span> to train the models using then plots the results. If you are interested, you can re-run both this section and the previous one to visualize the effect that the small training set size and random partition selections have on high bias/variance models. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Train the models</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>highBiasMdl = fitlm(data(tinds,[1,2]));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>HighVarMdl = fitlm(data(tinds,[1 2]),</span><span style="color: rgb(160, 32, 240);">'poly8'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the model and training data</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xplot = linspace(min(data.LevelChange(tinds)),max(data.LevelChange(tinds)))';</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>y1 = predict(highBiasMdl,xplot);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>y2 = predict(HighVarMdl,xplot);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(data.LevelChange(tinds),data.Outflow(tinds),</span><span style="color: rgb(160, 32, 240);">'rx'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>,2);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xplot,y1); plot(xplot,y2); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Change in water level (x)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Water flowing out of the dam (y)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>legend({</span><span style="color: rgb(160, 32, 240);">'Training Data'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Linear Model'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Polynomial Model'</span><span>})</span></span></div></div></div><h1  class = 'S0' id = 'T_2376D504' ><span>Regularized Linear Regression using </span><span style=' font-family: monospace;'>fitrlinear</span></h1><div  class = 'S2'><span>As discussed in ex5, the linear model is too simple to accurately reflect the complexity of the water flow data, while polynomial model tends to overfit the data and changes dramatically depending on the partition. In ex5 you found that the performance of the polynomial model can be improved on new data by adding regularization during training. Cross validation was then used to determine the 'best' value of the regularization parameter, </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>. In this section, we use the </span><a href = "https://www.mathworks.com/help/stats/fitrlinear.html"><span style=' font-family: monospace;'>fitrlinear</span></a><span> function to fit a regularized polynomial model to the data. Like </span><span style=' font-family: monospace;'>fitclinear</span><span> (used in the companion script for ex2), </span><span style=' font-family: monospace;'>fitrlinear</span><span> is designed to train models with a large number of variables and it also includes options for adding regularization. </span></div><h2  class = 'S1' id = 'H_94B12883' ><span>Load and normalize the data</span></h2><div  class = 'S2'><span>The code below will reload the data and create a (normalized) polynomial feature matrix </span><span style=' font-family: monospace;'>X</span><span> by combining the training and validation sets as before. As with </span><span style=' font-family: monospace;'>fitclinear</span><span>, </span><span style=' font-family: monospace;'>fitrlinear</span><span> requires feature and response data as numeric arrays (i.e. not in a </span><span style=' font-family: monospace;'>table</span><span>). Since we found that the 8th degree polynomial model was subject to very high variance in the previous section, we also limit the polynomial degree to 4 to outline a more realistic model example.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>load </span><span style="color: rgb(160, 32, 240);">ex5data1.mat</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>LevelChange = [X;Xtest;Xval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Outflow = [y;ytest;yval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>clearvars </span><span style="color: rgb(160, 32, 240);">-except LevelChange Outflow</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>mu = mean(LevelChange);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>sd = std(LevelChange);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>LevelChange = ((LevelChange-mu)/sd).^(1:4);</span></span></div></div></div><h2  class = 'S11' id = 'H_799DBC89' ><span>Partition the data</span></h2><div  class = 'S2'><span>In this section we will let </span><span style=' font-family: monospace;'>fitrlinear</span><span> partition the data automatically for us. We also provide code in this section and the next one to create the corresponding model using a </span><span style=' font-family: monospace;'>cvpartition</span><span> variable as comments.  Run the code below to select the proportion of data to hold out during training for validation purposes. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>holdout = 0.35;</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% c = cvpartiton(length(LevelChange),'HoldOut',holdout)</span></span></div></div></div><h2  class = 'S11' id = 'H_C63F5E20' ><span>Fit linear regression models using multiple </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(60, 60, 60);">λ</span><span> values</span></h2><div  class = 'S2'><span>In the companion script for ex3, we trained a regularized model by passing a single regularization value </span><span style=' font-family: monospace;'>lambda</span><span> to </span><span style=' font-family: monospace;'>fitclinear</span><span> using the </span><span style=' font-family: monospace;'>'Lambda'</span><span> option. We can also use this option with </span><span style=' font-family: monospace;'>fitrlinear</span><span>. However, by passing </span><span style=' font-style: italic;'>a vector</span><span> of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> values to these functions, a separate model will automatically be trained for each </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> value. Run the code below to create a vector of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> values and train the regression models for each </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>. When using multiple regularization values with </span><span style=' font-family: monospace;'>fitrlinear</span><span>, the trained models will be returned in a </span><a href = "https://www.mathworks.com/help/stats/regressionpartitionedlinear-class.html"><span style=' font-family: monospace;'>RegressionPartitionedLinear</span></a><span> model variable instead of a </span><a href = "https://www.mathworks.com/help/stats/regressionlinear-class.html"><span style=' font-family: monospace;'>RegressionLinear</span></a><span> variable which is discussed in the next section.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>lambda = [0 0.01 0.03 0.1 0.3 1];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>partMdl = fitrlinear(LevelChange,Outflow,</span><span style="color: rgb(160, 32, 240);">'Learner'</span><span>,</span><span style="color: rgb(160, 32, 240);">'leastsquares'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Regularization'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Ridge'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Lambda'</span><span>,lambda,</span><span style="color: rgb(160, 32, 240);">'HoldOut'</span><span>,holdout)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% partMdl = fitrlinear(LevelChange,Outflow,'Learner','leastsquares','Regularization','Ridge','Lambda',lambda,'CVPartition',c)</span></span></div></div></div><h2  class = 'S11' id = 'H_1A0C2528' ><span>Working with partitioned model variables</span></h2><div  class = 'S2'><span>The individual regression models are contained in the </span><span style=' font-family: monospace;'>Trained</span><span> property of a </span><span style=' font-family: monospace;'>RegressionPartitionedLinear</span><span> variable. In the case of hold-out cross validation, the </span><span style=' font-family: monospace;'>Trained</span><span> property is a cell array which contains a single </span><span style=' font-family: monospace;'>RegressionLinear</span><span> (or </span><span style=' font-family: monospace;'>ClassificationLinear</span><span> when using </span><span style=' font-family: monospace;'>fitclinear</span><span>) model variable. This variable contains the trained models for each regularization value </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>. The model partition that was used during training and validation is found in the </span><span style=' font-family: monospace;'>Partition</span><span> property. </span></div><div  class = 'S2'><span>    The model coefficients (</span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">θ</span><span>) are found in the </span><span texencoding="p\times l" style="vertical-align:-5px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEEAAAAjCAYAAADLy2cUAAAD1UlEQVRoQ+3Yeai1UxQG8N9nKiIZyvSHUhJ/kKREITIlZMhMkalIhoiQP2QoIYmMIbOkJEPIFCJJZlIyFEmUKfP0sN56v/Odc8/7nnv13e49u27nnPfuvfZez17rWc96l5gOS6YYmIKQIJhGwhSE/8hgGgmLDIRVsT62wA54FC8spkjYG9vjQGyGt3Aw3l9MIDS+Xoj8PYDj8O1iA2FN3IyDcAEuxl+LDYQtKwI2xe54shGKy6s6rIGTcD8+6aFa18PRuAXf9FiXqYfgXrxR3z9YniCsiHNxER7ByR2BCAA3YL8K50vxR0cgsmf2y7531J7fTwrCKkip+a7yaSXkVjNCMn92PNTauAzHdwRiI1xbANyEc3pGwrrl/J44A1e1zzkuHTbAoZVDm2Nj5AZiJA6cUM9i81WcipfnGIjZApDjbIOHEX92wvN9QGjmxkjydxOch61wK54uw7ml1OIXcSQ+niMg5gKAHOUI3ImXcPhg+o2LhMaX3fAEkkdvVhS813L0RFxfv5di3g5gjEqNNgDJ4zPxVQd7g1OSwkm904tT8vlT30gIULn9EMsXddOJgPZokM6zRMJdPQ87CERS7qzigNkAkGNsWOfZuSpSyHWp0SUS2iLjNpyCHwfspGzdPmEkNKbaQDTPZgtA7GxXUZzve1XK9gahLTKG3XKAbOToh6XIkjKTjJBvomhrvIPDSudPYqtZ06Tqs8UNn08SCfvioWEio4ytVeJlf4yKlC5OtDmgmd9HRwzbI+U8lSxA5DOl9de+ILRFxigHdywltjqOKsC6ON2eM0iCl1Q976ojRu2Xkn53dZBRqMvwQRaO44S2yBhmpI10REyYd5AvxgEyqgr0FVTD9sl7g8fwA/bBa8MmjQOhERm55ZS+thDK2uRsSuO7OAbtsjnO+fx/XBmcLRC5lCvxeEVpSvzZxTsfNQccB0JDKsNERl5SpDWNhE5v/nYXr1tz2r3ATFVgUiBWw9V1tvBB+oYDav80Uv+20ePSoR3qT5XSilhZAbvi8iLL8/HZ/wRAY3YSINapG9+jwHgdsXMNfm+fd6ZIaJPKM/gSaT8jmT8tMnylR9PU7BsQT8MV1dR0VYJtIHKrWf/bDOCH1I8toRcZH1J8EL/0qQ5h/edKKg/yQc+LX2Z6oix8khIYcLuOAJGQvmcCAh65x0yR0JDKSJHR9eTzfd4oEPKOIJ1h6v511bz8PN+dmfR8o0DIa+n7Kv9HioxJN51v64aBkGcpeTfWYXdBiHHBjkEQUlbSJW6LlcvrqK20zqmtXy9EJMaJpYXo8zI+TUH4B5K/AZ568SROmHOiAAAAAElFTkSuQmCC" width="32.5" height="17.5" /></span><span>  matrix </span><span style=' font-family: monospace;'>Beta</span><span> and the </span><span texencoding="1\times l" style="vertical-align:-5px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD4AAAAjCAYAAADMibkBAAAC0ElEQVRoQ+3YW8hNeRjH8Y8hzZRDOTRlLuSKISF33LhgSo2zmpxvxhAX4xA55GpymBpcKcKFnBrNjZlEoihSI2lIkisXTGM0pcYIiR79l7bXfre19n7be5l3r9pXe/0P3+f3PM/v/189dNOnRzfl1gbvbsq3FW8r/v+JQGRzPwzDSIzFT3gYiHlT/RNMwHfYijslj08fLMR4fIO+OIDv8V8e8Ax4DWbhjzRR2cEzXQbjGCYjGHZnf9RSfFSK2n1Mw1cfIXgo/hsiA6bich7w3nieXlyMQx8h+DLsxQUswIM84JVlHIOONAgedbYcJ3CvQI/4HBH4g/inwLhPsRMrsA+r8bTZ4D2xET/gFFbmhA/o2PQMbMF2vMwJPwRHMSkFPOZ5++Tt6l2h+ADswNKc8F9gT4Lejw0FFZ+I0/g39ahrrQKPdfPCNwoda0Vq78I5zMffrQTPA98V0J8l64rmFhYW2ZI16jf8zUz1yoB3pnwl9GGs7ahUzvoemvw7Dl1xkIlaf+dpFXg15aNxrUs13Qh0zD0FZ3EXc3GjTOAd4bO9NQod84SDbMMZLMKjsoHHfr5MqTgOtzAPN3OmdLXX4rwQbhDAkUVhg+9ZYCtTPTZdWdMZRBGfrwY+HD9jTCqbX6u91Erwjo0sUjMuEnl9vrOkmI6Tteq7lV29s+6d1+c7gw4hN6cT4i/4Fo/LoviHLKsR+P7p3h2dPGo7vh2EtcVZ/0c8y4LQ7FSvPHvX6t71wo9Il6DRqb7PY1O6YN0uenLrlSK3Hn9iJn6vo+vmhc6mrgc+zueX0gRLEI0u/PxiETsbhDn4Ov2ysVcR9XMlBeBt+tQIRnzJWZWuiUV8uhI+vDmumS9qrDMwiRQNLu7g8Y3tOl4VAa9D1JpD4vwcHh129VeByQN+No7jSYFxNV/NW+NdtV5p5mmDl0aKJm2krXiTAl2aZdqKl0aKJm3kNWSH0iTxLWtCAAAAAElFTkSuQmCC" width="31" height="17.5" /></span><span> vector </span><span style=' font-family: monospace;'>bias</span><span>, where </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">p</span><span> is the number of model coefficients (4 in the current example) and </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">l</span><span> is the number of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> values (6). The columns of </span><span style=' font-family: monospace;'>Beta</span><span> therefore correspond to coefficients for a particular model. The model coefficients and the corresponding lambda values are displayed below for comparison. Run the code below to extract the trained model variable, the partition variable, and the model coefficients. Do the increasing regularization values have the expected effect on the size of the coefficients and bias?</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>polyRegMdl = partMdl.Trained{1}</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>c = partMdl.Partition</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>[polyRegMdl.Bias; polyRegMdl.Beta]</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>lambda</span></span></div></div></div><h2  class = 'S11' id = 'H_D0E143AD' ><span>Evaluate model performance and select models using </span><span style=' font-family: monospace;'>kfoldLoss</span><span> and </span><span style=' font-family: monospace;'>selectModels</span></h2><div  class = 'S2'><span>Instead of using learning curves to detect high bias or variance issues, followed validation curves to determine the optimal regularization parameter value, we will use the </span><a href = "https://www.mathworks.com/help/stats/regressionpartitionedlinear.kfoldloss.html"><span style=' font-family: monospace;'>kfoldLoss</span></a><span style=' font-family: monospace;'>*</span><span> function to compute the cost of all models in the partitioned model variable using the validation set. We then extract the individual model with the lowest cost using the </span><a href = "https://www.mathworks.com/help/stats/regressionlinear.selectmodels.html"><span style=' font-family: monospace;'>selectModels</span></a><span> function. The result will be a </span><span style=' font-family: monospace;'>RegressionLinear</span><span> model variable corresponding to the best model as found by cross-validation.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>valCost = kfoldLoss(partMdl)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>[~,bestidx] = min(valCost)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'The best value of lambda found by cross-validation is: %g'</span><span>, lambda(bestidx))</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>bestRegMdl = selectModels(polyRegMdl,bestidx)</span></span></div></div></div><div  class = 'S12'><span>*K-fold cross validation is discussed at the end of this Live Script. K-fold validation with 1 fold is equivalent to hold-out validation.</span></div><h2  class = 'S11' id = 'H_93A4BC25' ><span>Predict the response for all models and visualize the results</span></h2><div  class = 'S2'><span>Run the code below to plot the training data, validation data, and the models for each </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>. Note, when using the </span><span style=' font-family: monospace;'>predict</span><span> function along with a model variable containing multiple models (coefficient sets), </span><span style=' font-family: monospace;'>predict</span><span> will return a response </span><span style=' font-style: italic;'>matrix</span><span>, where the </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">i</span><span>th column of the matrix corresponds to the prediction using the </span><span style=' font-style: italic;'>i</span><span>th model (trained with the </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">i</span><span>th regularization value).</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Extract the training and validation data</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xtrain = LevelChange(training(c),1); </span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ytrain = Outflow(training(c));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xval = LevelChange(test(c),1); </span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>yval = Outflow(test(c));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the data and models</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xplot = linspace(min(LevelChange(:,1)),max(LevelChange(:,1)))';</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xplot = xplot.^(1:4);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Y = predict(polyRegMdl,xplot);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xtrain*sd+mu,ytrain,</span><span style="color: rgb(160, 32, 240);">'go'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xval*sd+mu,yval,</span><span style="color: rgb(160, 32, 240);">'LineStyle'</span><span>,</span><span style="color: rgb(160, 32, 240);">'none'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Marker'</span><span>,</span><span style="color: rgb(160, 32, 240);">'square'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plt = plot(xplot(:,1)*sd+mu,Y,</span><span style="color: rgb(160, 32, 240);">'--'</span><span>); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>set(plt,{</span><span style="color: rgb(160, 32, 240);">'Color'</span><span>},num2cell(jet(length(lambda)),2));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>set(plt(bestidx),</span><span style="color: rgb(160, 32, 240);">'LineStyle'</span><span>,</span><span style="color: rgb(160, 32, 240);">'-'</span><span>,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>,2);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylim([min(Outflow) max(Outflow)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Change in water level (x)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Water flowing out of the dam (y)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>legend([</span><span style="color: rgb(160, 32, 240);">'Training'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Validation'</span><span>,cellstr(</span><span style="color: rgb(160, 32, 240);">"lambda = "</span><span>+string(lambda))],</span><span style="color: rgb(160, 32, 240);">'Location'</span><span>,</span><span style="color: rgb(160, 32, 240);">'NorthWest'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>title(sprintf(</span><span style="color: rgb(160, 32, 240);">'Best lambda = %g'</span><span>,lambda(bestidx)));</span></span></div></div></div><div  class = 'S12'><span></span></div><h1  class = 'S0' id = 'T_9F8A9BD8' ><span>Hyperparameter Optimization</span></h1><div  class = 'S2'><span>Model parameters, such as the regularization strength or optimization solver settings, are commonly referred to as</span><span style=' font-style: italic;'> hyperparameters</span><span>. It is often the case that we do not want to explore the effect of hyperparameters on a model; rather we are only interested in obtaining </span><span style=' font-style: italic;'>the best</span><span> model and/or hyperparameter value as judged by cross-validation. In these cases we can utilize the </span><span style=' font-style: italic;'>hyperparameter optimization</span><span> options in </span><span style=' font-family: monospace;'>fitrlinear/fitclinear</span><span>, which then automatically return the best model found after training over a set or range of hyperparameter settings. In this section we use hyperparameter optimization with </span><span style=' font-family: monospace;'>fitrlinear</span><span> to automatically obtain the best cross-validation strength and the corresponding model.</span></div><h2  class = 'S1' id = 'H_2B6854DA' ><span>Specify the cross-validation method</span></h2><div  class = 'S2'><span>To use hyperparameter optimization, the fist step is to create a </span><span style=' font-family: monospace;'>structure</span><span> variable that contains information about the cross-validation method that will be used to judge model performance. We can then provide a list of parameters to optimize using the </span><span style=' font-family: monospace;'>'OptimizeHyperparameters'</span><span> option when training the model and providing a cell array of parameter names to be optimized. If desired, we can also adjust the permissible range or values of hyperparameters to be searched during optimization from their default values by obtaining the corresponding </span><a href = "https://www.mathworks.com/help/stats/optimizablevariable.html"><span style=' font-family: monospace;'>optimizableVariable</span></a><span> structure for the given machine learning function, method, and dataset by using the </span><a href = "https://www.mathworks.com/help/stats/hyperparameters.html"><span style=' font-family: monospace;'>hyperparameters</span></a><span> function. Below we obtain the variable for </span><span style=' font-family: monospace;'>fitrlinear</span><span> with least-squares regression and the current dataset, then select only the first optimizable parameter, </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>. </span></div><div  class = 'S2'><span>    Run the code below to create an options structure for hold-out cross-validation using the same percentage as in the previous section, then create a hyperparameter options variable with a custom range of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> values to search.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>opts = struct(</span><span style="color: rgb(160, 32, 240);">'HoldOut'</span><span>,holdout);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>optimizableParams = hyperparameters(</span><span style="color: rgb(160, 32, 240);">'fitrlinear'</span><span>,LevelChange,Outflow)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>optimizableParams = optimizableParams(1)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>optimizableParams.Range = [1e-4, 1]</span></span></div></div></div><h2  class = 'S11' id = 'H_A8A3614D' ><span>Automatically select </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(60, 60, 60);">λ</span><span> using hyperparameter optimization</span></h2><div  class = 'S2'><span>By including the validation options structure and the hyperparameter options variable created in the previous section and naming the </span><a href = "https://www.mathworks.com/help/stats/hyperparameters.html"><span>hyperparameter to be optimized</span></a><span> in the call to </span><span style=' font-family: monospace;'>fitrlinear</span><span> or </span><span style=' font-family: monospace;'>fitclinear</span><span>, these functions will automatically search through the given parameter values and return a model variable corresponding to the best </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> found as judged by cross validation. Since many models (30 by default) are trained during the search, this will take considerably longer to run than when training a single model. During training, the progress is displayed. If desired, the fit information for the final model, along with the hyperparameter optimization information, can also be obtained by providing additional output variables. See the </span><a href = "https://www.mathworks.com/help/stats/fitrlinear.html#bu216n7_sep_shared-HyperparameterOptimizationResults"><span>documentation</span></a><span> for details.</span></div><div  class = 'S2'><span>    Run the code below to call </span><span style=' font-family: monospace;'>fitrlinear</span><span> with hyperparameter optimization for the regularization strength. The final model variable will be displayed, along with the best </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span> value found (you may have to scroll past the progress plots in the output). The best model found in the previous section using a fixed set of parameters and manual model selection will be plotted alongside the optimized model computed below for comparison. Which model performs better?</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>optimRegMdl = fitrlinear(LevelChange,Outflow,</span><span style="color: rgb(0, 0, 255);">...</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>                       </span><span style="color: rgb(160, 32, 240);">'Learner'</span><span>,</span><span style="color: rgb(160, 32, 240);">'leastsquares'</span><span>,</span><span style="color: rgb(0, 0, 255);">...</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>                       </span><span style="color: rgb(160, 32, 240);">'Regularization'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Ridge'</span><span>,</span><span style="color: rgb(0, 0, 255);">...</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>                       </span><span style="color: rgb(160, 32, 240);">'OptimizeHyperparameters'</span><span>,optimizableParams,</span><span style="color: rgb(0, 0, 255);">...</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>                       </span><span style="color: rgb(160, 32, 240);">'HyperparameterOptimizationOptions'</span><span>,opts)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xtrain*sd+mu,ytrain,</span><span style="color: rgb(160, 32, 240);">'go'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xval*sd+mu,yval,</span><span style="color: rgb(160, 32, 240);">'LineStyle'</span><span>,</span><span style="color: rgb(160, 32, 240);">'none'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Marker'</span><span>,</span><span style="color: rgb(160, 32, 240);">'square'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plt = plot(xplot(:,1)*sd+mu,Y(:,bestidx),</span><span style="color: rgb(160, 32, 240);">'b'</span><span>,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>,2); </span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xplot(:,1)*sd+mu,predict(optimRegMdl,xplot),</span><span style="color: rgb(160, 32, 240);">'r'</span><span>,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>,2); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylim([min(Outflow) max(Outflow)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Change in water level (x)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Water flowing out of the dam (y)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>legend(</span><span style="color: rgb(160, 32, 240);">'Training'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Validation'</span><span>,sprintf(</span><span style="color: rgb(160, 32, 240);">'lambda = %g'</span><span>,lambda(bestidx)),</span><span style="color: rgb(160, 32, 240);">'Optimized lambda'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Location'</span><span>,</span><span style="color: rgb(160, 32, 240);">'NorthWest'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>title(sprintf(</span><span style="color: rgb(160, 32, 240);">'Best lambda estimated by hyperparameter\noptimization: %g'</span><span>,optimRegMdl.Lambda));               </span></span></div></div></div><h1  class = 'S0' id = 'T_DB0B4D05' ><span>K-Fold Cross-Validation</span></h1><div  class = 'S2' id = 'H_031D0AE4' ><span>This section discusses the most common type of cross validation used with MATLAB machine learning functions - K-fold cross-validation - which is not used in the programming exercises. </span></div><h2  class = 'S1' id = 'H_8402A8B0' ><span>Introduction</span></h2><div  class = 'S2'><span>Recall that the cross-validation technique used in the programming exercises is referred to as 'hold-out' validation, as the validation examples are 'held out' of the training process. This method is most often used when there is a large number of training examples relative to the number of parameters in the model. For smaller datasets like the water flow example, the model coefficients and optimal parameter values can depend strongly on how the training and validation sets are distributed and the random nature with which they are selected. There are several </span><a href = "https://www.mathworks.com/discovery/cross-validation.html"><span>alternative cross-validation methods</span></a><span> available in MATLAB that are preferable in situations where there is not much data available. The most common is referred to as </span><span style=' font-style: italic;'>K-fold</span><span> cross-validation, </span></div><div  class = 'S2'><span>The idea behind K-fold cross-validation is the following:</span></div><ol  class = 'S3'><li  class = 'S4'><span>Start with a data set, </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">X</span><span> and choose a number of 'folds', </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">k</span></li><li  class = 'S4'><span>Randomly select </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">k</span><span> equally sized subsets (folds) of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">X</span><span>: </span><span texencoding="X_1,\ldots, X_k" style="vertical-align:-6px"><img src="data:image/png;base64,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" width="61" height="19.5" /></span></li><li  class = 'S4'><span>Denote </span><span texencoding="X_1" style="vertical-align:-6px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACIAAAAnCAYAAABqippxAAACcElEQVRYR+3XSeiNURjH8c8/U5FEZNhZWtiyUIZkyoYkZB6KhSEpG2EhmZLIkClkWliwI8NCRKywYUUWQomFJGXqqfPW29v9X+e+f+qv7lu39w7nnOd7f+d5nt95O3STq6ObcGiDVHeirch/p8hkzE2vQSX617iCC3iavh+FZViFYuxHHMQ+fM2pzGY5Er/Nxln0T4tNxa0GC/fBHmzAzXR/kQNQjPlTsvbD4fSPY84iXGwQIFQ5g3dYjfetQMTYP4HEmJU4lRYOddbiSynQUBzHMCzH81YhckFG4jLG4i1m4XEKFortxUwsxd06ELkgPbAdW1OQTTiA+D7eb8GaBPvrX4LE2mNwDcNxO/37iQkoYC7hZ12IXEVi3ECcTlUUn09gGo5hP753BaIVkGrSxuej2FxJ3No8OVVTLF5O2peYgye1I1cmtgIyAEewsJK0tRO0zJIL0jNVyO7S5BtYjA9/Q5UckBizIFXILkxIveRzSt47GSBDUscN9XY2Gp8DEoHPlSokGlfRaQ+lhP3WCUw0vFAtrGEctmFHHZDCQ66nDhpOWk7aR0mtV52A9EKoEF35fl2QwkPeVMo0nDba+voUPOw/ekyzK9SoBRK2H7Y+ohM3jfPK1XQ8iHsY46cmJLVACiObhPl41iDAYJzH9AZG2IinZZBQIrJ6XTpfxL1s+UWQvukEFtsS10lsbNJls0F6Y0na99Fp8SjP8JQwu4f4kbZiBqZgXunkFlPuIXpLHJziSFm+skEyWkGXhrRBqvK1FakqMj6dZ2u3+C5laKqq2JYV6WEtLCEa5YPqI0eO6XUVJmt+G6QqU1uRbqvIb57ygihavPuyAAAAAElFTkSuQmCC" width="17" height="19.5" /></span><span> as the cross-validation set: </span><span texencoding="X^{cv}_{1}" style="vertical-align:-8px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAArCAYAAADsQwGHAAADkElEQVRYR+3YWehtcxQH8M81JCEhiVISIQ8eZCghQ4YMGZPxurd7Q2SMF/P0QCHygBfzlExlnqeIyBOeJA+mMkXGTH219u047XPsvTv3X7fO7+Wcfc5v/X7f33et9V3rtxdZBceiVRCzOeiF8tqc6TnTUxiYFh4bYTEOw+5jazyBx3EvfsFaOAbHYf+Rua/jXLw7Sy90iekNcSuOqo3vxun4sQXITsiB1saFuA2/zRJw1uoCOvP2waNYD8/gRHw9BmYNnFdgT8X9+HvWgPuA3hR3Yd9i+HC8OAIohz8Wt+BqXIc/WgBn3rY4AL9jKzyGD7EzdsBmuBjflccuqnWuqlDszHQ2O6fAZI2bcMGI6/fEnXiyfv+pBXBCJp7YHAHyA65FbJMPH9eBD8HR+ACN95JbzW+dQQfD9ngI2+HtYvaTer4dX+IUfNUCuNn8ICwpgKvX9yR8cuZ7HFoJvh+er3XimbMq7L7tEx6ZG4UIM2fWYssq6bJhRpLzswkxvAceKCYTQn9OmLcrnsP5dZB4eGmFxYoc6ZqIzR5H4OF6eBnflLvD3kcTgITRK0t9EgbvT0nObfAgnqq43rJAX1Oe+Ne0L+jRhIz9p6Xlr04BsgEik+vgeHw+ZW6SMNr/FgL0tFKh7LNi9AU9npAJl+hxm1I0m0wCnbXWRZL2r5qc+A7on/FeyWs+/zP6go7xgeW+fB9NyEkEJjwuLXdHKV7DakioRVESq82hG9Bb4Gw826b1fUFHOaIUu4wgPKHYmeJ1G+MKRD2iQL/iabw5wnLs18f1uA8vTSpOfUBvUhmdxbPgjYXyDpxRbp4GfGb/dQWdJEr87ljaGk1tKuQX1VS9MzNU/7NQF9BNYUjpjhYnMcYTMpXuhpXVa/RNxKanuAzLMSpt6RXSN0QGX8BJCOsrfUxjOv/tVU1QQI93bQmZm3FyoTwSj3RAnKRMuU8HmOaq95gGummCopuRrDYtTrG4p3Ydb6LGweSQaWmjNrvhkqqUMwGdgwRweoSU1b2Rkt020lM0IZPQCKAoS9tYs9iNXL4xS9Bb43IcXA1/Nn+lmp30HE3jn74316rcZnJbaUa6sOhwYjyXhrbGKCzPFHRvVw0wmIMeQNogkznTg2gbYDRnegBpg0zmTA+ibYBRU0VnWsYH4OhkkldqCY28EsiLl1zVcnnN7aXtXcnERbv0050QLeSkOeiFYnvO9JzpKQz8A0/qxSwn04ddAAAAAElFTkSuQmCC" width="22.5" height="21.5" /></span><span> and combine the other subsets into a single training set, </span><span texencoding="X^{t}_{1}" style="vertical-align:-8px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAArCAYAAAD/lEFzAAAC8klEQVRYR+3XTcgVdRQG8J+khkgESYgLlyIR1EJSMDTUQA2RTEVKTY3EwBBUcCNoIkIJZkaQRZAfhUppCqYu8iOKKMm1tAoXoklfoiGCohw4V6Zx7n1nvO9C5f53986Zc57/cz6eMwPch2fAfYhJD1TdrPSYemiZGoZFeBkTSrc8jEP4EtfwKObhNUwt2P6AVfi1LkthV6emnsAnmJOOd2M5rlQEeg4BeAjW4lNcL9hFvFfzEp+3A1oHVLw7Bd/gMRzDQvxZcjoQqxPMW9iDWyVA07ARb+PnbkGNwC68mAzNwvEKBrZjE7bgRuH5U1iB+biKYPsoTlUBq8tU2K3MYOHnQ6wppOYF7MS3+f9/FcFGY1/arC+B/p95XVDx0tP4CnHrX7I2fs/fUR8XsQx/tEnLzGyOV7IU2tZ+E1DRYZszDeHwzSzqaII4Ufzn20R6JGvppezS3zp1YxNQ4SduuT8dnsRfGIklONsh0OP4LNMd4C/3J6hiwYffcznLvu9jDj2DrxFpDrZv9ieocsFHgJhHxU6rihdd93HOukt5mX+6HQnF96fjSP5RLPh2MQbjXYzHBzk4QwnaXqRpTUXnRQrGFRAsSLnpBCoGZjTG+wmsamTceb8JqOEpN/HyCWxLLztyQncM1EfN3dOcGpoFOiY77d/ChL+Qon26SeBuC72laSEt0c5nUsiLEz40b2tJ6+4ZY1/pa6n6O1iKYuuPxUHEmPgOryNY6/p0AhXPJiFENkCVVT9S+hEWJ4rZOFAD0ZMpR7FBhHjfdTqBaolstG87AY3580V6LYt0OVhcIlae6NbnsS6lpxaoABqAgqFQ9skISak6EwspjdRFwOjMqjMoay7GyY9NQI3CBszIhS6cx86zNzWvtdg9m2tvbKOxbbbO37lJRI3FUlglJ8FSI1A1SqJrkx6ouhT2mOoxVZeBuna9mqrLVEsFGslMXedN7eKTP1L3Bubmt+N7+Kn8rdjX6tI0cL/Y90DVpbHH1APN1G2WcqEs7zLg/QAAAABJRU5ErkJggg==" width="18.5" height="21.5" /></span><span> </span></li><li  class = 'S4'><span>Train and cross-validate a model using </span><span texencoding="X^{t}_{1}" style="vertical-align:-8px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAArCAYAAAD/lEFzAAAC8klEQVRYR+3XTcgVdRQG8J+khkgESYgLlyIR1EJSMDTUQA2RTEVKTY3EwBBUcCNoIkIJZkaQRZAfhUppCqYu8iOKKMm1tAoXoklfoiGCohw4V6Zx7n1nvO9C5f53986Zc57/cz6eMwPch2fAfYhJD1TdrPSYemiZGoZFeBkTSrc8jEP4EtfwKObhNUwt2P6AVfi1LkthV6emnsAnmJOOd2M5rlQEeg4BeAjW4lNcL9hFvFfzEp+3A1oHVLw7Bd/gMRzDQvxZcjoQqxPMW9iDWyVA07ARb+PnbkGNwC68mAzNwvEKBrZjE7bgRuH5U1iB+biKYPsoTlUBq8tU2K3MYOHnQ6wppOYF7MS3+f9/FcFGY1/arC+B/p95XVDx0tP4CnHrX7I2fs/fUR8XsQx/tEnLzGyOV7IU2tZ+E1DRYZszDeHwzSzqaII4Ufzn20R6JGvppezS3zp1YxNQ4SduuT8dnsRfGIklONsh0OP4LNMd4C/3J6hiwYffcznLvu9jDj2DrxFpDrZv9ieocsFHgJhHxU6rihdd93HOukt5mX+6HQnF96fjSP5RLPh2MQbjXYzHBzk4QwnaXqRpTUXnRQrGFRAsSLnpBCoGZjTG+wmsamTceb8JqOEpN/HyCWxLLztyQncM1EfN3dOcGpoFOiY77d/ChL+Qon26SeBuC72laSEt0c5nUsiLEz40b2tJ6+4ZY1/pa6n6O1iKYuuPxUHEmPgOryNY6/p0AhXPJiFENkCVVT9S+hEWJ4rZOFAD0ZMpR7FBhHjfdTqBaolstG87AY3580V6LYt0OVhcIlae6NbnsS6lpxaoABqAgqFQ9skISak6EwspjdRFwOjMqjMoay7GyY9NQI3CBszIhS6cx86zNzWvtdg9m2tvbKOxbbbO37lJRI3FUlglJ8FSI1A1SqJrkx6ouhT2mOoxVZeBuna9mqrLVEsFGslMXedN7eKTP1L3Bubmt+N7+Kn8rdjX6tI0cL/Y90DVpbHH1APN1G2WcqEs7zLg/QAAAABJRU5ErkJggg==" width="18.5" height="21.5" /></span><span> and </span><span texencoding="X^{cv}_{1}" style="vertical-align:-8px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAArCAYAAADsQwGHAAADkElEQVRYR+3YWehtcxQH8M81JCEhiVISIQ8eZCghQ4YMGZPxurd7Q2SMF/P0QCHygBfzlExlnqeIyBOeJA+mMkXGTH219u047XPsvTv3X7fO7+Wcfc5v/X7f33et9V3rtxdZBceiVRCzOeiF8tqc6TnTUxiYFh4bYTEOw+5jazyBx3EvfsFaOAbHYf+Rua/jXLw7Sy90iekNcSuOqo3vxun4sQXITsiB1saFuA2/zRJw1uoCOvP2waNYD8/gRHw9BmYNnFdgT8X9+HvWgPuA3hR3Yd9i+HC8OAIohz8Wt+BqXIc/WgBn3rY4AL9jKzyGD7EzdsBmuBjflccuqnWuqlDszHQ2O6fAZI2bcMGI6/fEnXiyfv+pBXBCJp7YHAHyA65FbJMPH9eBD8HR+ACN95JbzW+dQQfD9ngI2+HtYvaTer4dX+IUfNUCuNn8ICwpgKvX9yR8cuZ7HFoJvh+er3XimbMq7L7tEx6ZG4UIM2fWYssq6bJhRpLzswkxvAceKCYTQn9OmLcrnsP5dZB4eGmFxYoc6ZqIzR5H4OF6eBnflLvD3kcTgITRK0t9EgbvT0nObfAgnqq43rJAX1Oe+Ne0L+jRhIz9p6Xlr04BsgEik+vgeHw+ZW6SMNr/FgL0tFKh7LNi9AU9npAJl+hxm1I0m0wCnbXWRZL2r5qc+A7on/FeyWs+/zP6go7xgeW+fB9NyEkEJjwuLXdHKV7DakioRVESq82hG9Bb4Gw826b1fUFHOaIUu4wgPKHYmeJ1G+MKRD2iQL/iabw5wnLs18f1uA8vTSpOfUBvUhmdxbPgjYXyDpxRbp4GfGb/dQWdJEr87ljaGk1tKuQX1VS9MzNU/7NQF9BNYUjpjhYnMcYTMpXuhpXVa/RNxKanuAzLMSpt6RXSN0QGX8BJCOsrfUxjOv/tVU1QQI93bQmZm3FyoTwSj3RAnKRMuU8HmOaq95gGummCopuRrDYtTrG4p3Ydb6LGweSQaWmjNrvhkqqUMwGdgwRweoSU1b2Rkt020lM0IZPQCKAoS9tYs9iNXL4xS9Bb43IcXA1/Nn+lmp30HE3jn74316rcZnJbaUa6sOhwYjyXhrbGKCzPFHRvVw0wmIMeQNogkznTg2gbYDRnegBpg0zmTA+ibYBRU0VnWsYH4OhkkldqCY28EsiLl1zVcnnN7aXtXcnERbv0050QLeSkOeiFYnvO9JzpKQz8A0/qxSwn04ddAAAAAElFTkSuQmCC" width="22.5" height="21.5" /></span><span> to provide a set of model coefficients, </span><span texencoding="\theta_1" style="vertical-align:-6px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAnCAYAAAAYRfjHAAACVUlEQVRYR+3WS6hPYRQF8J+IJBl5ToRE8kjJYyAGBgwQkZBHUUJ5lkQoSqKQUCIlr7zCwMwzkSFFMlEGyAjJYyLa9f31d5zjnuP+r5vcM/7OWnutvb69v3Za6WvXSrzaiP+a8/+M1R0wHGPRFY9xGx+rWlVWcZwbh/24gz0YiuN4iGV4U4W8DHGcmYG9uIYNSWEoPoQFWIMD+FaWvAzxBJzA80Tysg58C7bjIpbifaOIB+IkhiTSqxngGvF9zMOLRhB3wW6swIXUx7cFxI8wB88aQTw9qQ2s6GNWbUfswlo0jLgHjmIaLmMJsmo7Y19yIq7UfLxqruKZuJRA1iWCLGZ3nMGklPZwJVtcYR15qY7eHsRiPMVsPMlBGIzzGIbDWI8vzVEckymuRy3RK/EhBzDaUOt7kSuVFEevTpWtPBUVA+ZGhX9+2cf1SQ2VU3AvB7AbjmEWrmMhXjeHuB7wd0kdjSvonXob6a4fl5GTyMgAbMbnbFHZcNUn9Ui6o9mf2mMbYmplw9cJU9P1m4wijF+sHoRzGIGt2JFjXz+cxRhsStPtazoXazPE9E84DwqKr0wcamM77cStnKVRq7MmoDRxH5zGxALFo9JVC4JFaTfnZaoycfQoFsOqlNrV+JSQe6aejUfc7WhJ0f6tTBwcNVXv6jZOr1RQDJeYUDebWPp/RBzhGImN6V0V47JvWhYxqcq8r/6IuMocKDrbRtzk5GqEzYHRalbXdnU8AuNp1OKKYw7EApmL5enVGXP9bnoe/3C0zLu6Ufb/hNNG3CK25oH+f1Z/B/1rlyjEZEi4AAAAAElFTkSuQmCC" width="15" height="19.5" /></span><span>.</span></li><li  class = 'S4'><span>Repeat steps 3 and 4 with </span><span texencoding="X_i" style="vertical-align:-6px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAnCAYAAAAYRfjHAAACq0lEQVRYR+3WS+hVVRTH8Y8kCoWIIpjNHPgoCWc6cGAi+UAQpSTFZygklEUogphEQfhCxFehJSn5QB2kI8V0IBJqTqqBCYISISr4GIiIYClL1pHj5XjvudrNQf8Nf87/3rP2+t699m89unlBq9sL4uoC/2eR7wp1EeqxmJZ/fUvx/xMHsAu/5fevYx4WoLC9iQ1Yi7tV99cs1PFuKnagV24eh58qHPXEanyCo/k830wwre74FWzOE4WfWdhd4TBO/T2u4gNca6XSVuDYPx/fpaM4/Ue4U3LcH1vxKt7HH62g8b4OeCD2YiSuYAp+SecRkTWYhLk4UQdaF/wSPseKdLoY6xHfx//LsTB/3IN/Exy+RuAgBuBYnu6t/AEB34N/6kLrnjjs+mB7qjw+b8N4fIN1uN8OtB1wo8ji89dY2iC02vw64iqclUV2Ee/g19qkBsN2wL2xBTMbRFZbUGV2XXD3VPCq0uYjmI3rz3LqOuCwmZEKXonRmcu3U2zHOwUO0M6SgqNQFJVsYwrsXhP4IPyAs2n7qGm0OnFRgw9nhYpNZZGdyWhcagJ+I/P853IWNAMXNfhyQ9pEJ4oy+XHCoh1Gjre1ngaONhht7rWndJvo1z9mu4xnNJJb7ZCrwEXhH4Pp+L3CYb+8twkVjaMwD5toLMMRV/YlLhQvG8Fx0q+wKPtrPMstsNj3ck4YEeZY3+LTCtuh2I9z+BA3GsE9MCfv7c18GekSNTmawyn8naGdiLfxXmkyiS0nEbkdg0KMSLFGIYQZaRi6CB+PVitVt3NtVbYRhS/wbo5Ej206CY5rixI7uCrlOgkegn04nff/xLTZSfBkHMoBMYaHuN/Hdb2T4GU5/C3BsBwIY97uuLiipm/KQvMZ/iqrr5MnbpoRXeDnLRi19///Qv0Q+yCCKI/n7KoAAAAASUVORK5CYII=" width="15" height="19.5" /></span><span> as the chosen validation set for </span><span texencoding="i = 2,\ldots,k" style="vertical-align:-5px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIwAAAAjCAYAAABGiuIFAAAFhElEQVR4Xu2aZ8i2YxjHf689QvZIyPbBSvmAUDYZZW9lZmVvKbPsESHKDGWEbB/sKD4gISESMrMysvr1Hufreu/3Gvf9PPe47tt51tPz9FzndZ3HeZz/8xj/45hBHlkDPWhgRg9z89SsATJgMgh60kAGTE/qypMzYDIGetJABkxP6sqTM2AyBtTAgsAqwPzA28BfVWppAsw8wPHAKcBV8fNny3W8PLAfsAOwMfAd8BxwG/AS0Hb5h6XetYETgE2AdWPRk+KMK2VoAsySwN3AdsCbwN7A+8PaUY/ruBflvBpYq+Lda4CzgV96/PYkT98ceD42uC3wTN1mmwCjqboUOBa4FRCBP7ZUe+sA9wCvhDX5EFgG2BU4FVgi5PZWXQv809J9DFssjcC9oTct8yfTAYzvzgUsEreyreZc33sR8DVwRYnb2QK4HVgZeBY4CPhi2CfTwvU0GOfFz03AicCv0wVMC/c5h0i6oLPCknxVIvDcoZRzAyg7A2+Mw8YGLONiwC3AHsBRgKCpHU0uqen9tjzfBVg63GaVTNsAT8fDzYCX2yL8COVYD7gfWC6ShEad1AFGV2TQazS9KvAI8P0INzfdpTeNLMm0cS/gvel+cALeT/GLWeT+wOdNeyoDzEKRlu4DbBUf6Mq/NS024ufJwnijDgN+GLE8o15eN30BcGak0mcA0ih7hrVZHPg5LNCLwGcKXGdhNgAeCOvSlX8raMBY4fxpaqTfabwB3ZUBFjO+//tYCrgT2B44GLgjFCIm1JXnfxrwZVFRdYAxLnh4ikFi2wDjbUkgOTKyqf87YDYCHg0lpCQgcVnGNteVZUxVgCmmW08CBwLfjLGG5WIuAw4vkFRjvJ2+iO7FuRFI5ysjvluUCe6rYsSrAFNMty4BtBiV9YW+iD+4j1gjsSxwcxB7mbCbWTuy1CNoPN8L4+93gafqSM0qwKR0a41gSs2QxnEsHEz1pxWE3jjuqR8yrxAlny2Du/K81wT2BT6uW6AKMCnd+iBInbf6IeWQv2HEf3KsWcb+DlmcVi0nxfBEMPhFwcweaxOCMsAU062ppqCjDnoTWBYIC1NLd7fqKIcjTMoYX4uKta7pEOAh4NA6vq0MMGYUpls7RexijaZXvz9KwAgWaYDVgHMqKtPu2/rTb8M5n1atYl3w+khkUnxqUiCF4tgdeLBK4jLAFOMXy90W6w4IKv2jVm19TmEEi5zChsDpNW0MVrYt6xsIexnUw6LxOavx3V4Q31sRkOzUff9doZ9+z+tcZr5wLz8BfzSckXU3s6D1C/GpPUTyMFvH7+OiK8Firc9eTd8sA0xiRG0TkC5WuZYHzMvbWq12P5YyLM/bE6OFU3mdwzkSUrY4SEq9E2Ax2DPF9Pb10jMjf2EGNi+gkq2Il41+zyuusWwUDbUS9iodAbxQA5p0vkViVBwYv3iB1Js0ymNR0H0ceL0OMILkrmiY0kytFC/aOtDW4YaLh94kp/0wAub34B1Siul7KsxuvcZCXNDqF8diN0SQXebmpN/7Oa+4v1QjS/+rK+OoJxvILAkYdhxTuFidwLPmpmWxIW0WpVJmYVaPW6Nl8daZYUjqtHnsGApITVJNsnobE1WgDqyf2CimCXbogu00bBqad7MKXYKtrLNMd8eL/Z7XaWE8Iy+6Qx7Fv78tEb7Ir5W1Y0qjWD6xbfPyAMtsCcOktDc0HWy3z41FtDb6824sTLffHdY84zLdcmMj1FQFyoCZXXNa1aPDbLe1FbXqrA34dbPGG6nvZ6q4qHwvA+Y/1ejDpRA071Lk4zRSDGfGpksZWHKSATMTFtabtCzGbG2nDjqBbOYnlWBKbzO3gfzARgbMwFQ7mR/OgJnMcx3YrjJgBqbayfxwBsxknuvAdvUvWbMYMyOcAt8AAAAASUVORK5CYII=" width="70" height="17.5" /></span><span>, until there are </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">k</span><span> sets of trained coefficients, </span><span texencoding="\theta_1,\ldots,\theta_k" style="vertical-align:-6px"><img src="data:image/png;base64,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" width="57" height="19.5" /></span></li><li  class = 'S4'><span>Average the </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">k</span><span> sets of coefficients to produce a final set of model coefficients, </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">θ</span></li></ol><h2  class = 'S1' id = 'H_A6D1734D' ><span>Using K-fold cross-validation instead of hold-out validation</span></h2><div  class = 'S2' id = 'H_6D4656BD' ><span>In the following section we outline two ways to incorporate K-fold cross-validation instead of hold-out cross-validation when using MATLAB machine learning functions. Sample code is provided for each method, which can be used in the first part of the script if you are interested in trying out K-fold cross validation for yourself.</span></div><h4  class = 'S13' id = 'H_6B33E428' ><span style=' font-weight: bold;'>Create a K-fold </span><span style=' font-weight: bold; font-family: monospace;'>cvpartition</span><span style=' font-weight: bold;'> variable for use with </span><span style=' font-weight: bold; font-family: monospace;'>fitlm</span><span style=' font-weight: bold;'> or </span><span style=' font-weight: bold; font-family: monospace;'>fitglm</span><span style=' font-weight: bold;'>:</span></h4><div  class = 'S2'><span>The code below will create a </span><span style=' font-family: monospace;'>cvpartition</span><span> variable for K-fold cross-validation:</span></div><div class = 'preformatted-matlab' style = 'margin: 10px 3px 10px 55px; padding: 10px 10px 10px 5px; '><div  class = 'S14'><span style="white-space: pre;"><span>cvpartition(m,</span><span style="color: rgb(160, 32, 240);">'KFold'</span><span>,k); </span></span></div></div><ul  class = 'S3'><li  class = 'S4'><span style=' font-family: monospace;'>m</span><span> is the number of examples in your data set</span></li><li  class = 'S4'><span style=' font-family: monospace;'>k</span><span> is the desired number of folds</span></li></ul><h4  class = 'S13' id = 'H_C3CA5B8E' ><span style=' font-weight: bold;'>Use K-fold cross-validation with </span><span style=' font-weight: bold; font-family: monospace;'>fitrlinear</span><span style=' font-weight: bold;'> or </span><span style=' font-weight: bold; font-family: monospace;'>fitclinear</span></h4><div  class = 'S2'><span>Use the </span><span style=' font-family: monospace;'>'KFold'</span><span> option with the desired number of folds:</span></div><div class = 'preformatted-matlab' style = 'margin: 10px 3px 10px 55px; padding: 10px 10px 10px 5px; '><div  class = 'S14'><span style="white-space: pre;"><span>fitrlinear(LevelChange,Outflow,</span><span style="color: rgb(160, 32, 240);">'Learner'</span><span>,</span><span style="color: rgb(160, 32, 240);">'leastsquares'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Regularization'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Ridge'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Lambda'</span><span>,lambda,</span><span style="color: rgb(160, 32, 240);">'KFold'</span><span>,k)</span></span></div></div><h4  class = 'S13' id = 'H_84F2EF93' ><span style=' font-weight: bold;'>Use K-fold cross-validation and hyperparameter optimization with </span><span style=' font-weight: bold; font-family: monospace;'>fitrlinear</span><span style=' font-weight: bold;'> or </span><span style=' font-weight: bold; font-family: monospace;'>fitclinear</span></h4><div  class = 'S2'><span>Create an options structure with the </span><span style=' font-family: monospace;'>'KFold'</span><span> option and number of folds, then pass it to a training function using the </span><span style=' font-family: monospace;'>'HyperparameterOptimizationOptions'</span><span> option:</span></div><div class = 'preformatted-matlab' style = 'margin: 10px 3px 10px 55px; padding: 10px 10px 10px 5px; '><div  class = 'S14'><span style="white-space: pre;"><span>opts = struct(</span><span style="color: rgb(160, 32, 240);">'KFold'</span><span>,k);</span></span></div><div  class = 'S14'><span style="white-space: pre;"><span>optimizableParams = hyperparameters(</span><span style="color: rgb(160, 32, 240);">'fitrlinear'</span><span>,LevelChange,Outflow);</span></span></div><div  class = 'S14'></div><div  class = 'S14'><span style="white-space: pre;"><span>optimRegMdl = fitrlinear(LevelChange,Outflow,</span><span style="color: rgb(160, 32, 240);">'Learner'</span><span>,</span><span style="color: rgb(160, 32, 240);">'leastsquares'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Regularization'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Ridge'</span><span>,</span><span style="color: rgb(0, 0, 255);">...</span></span></div><div  class = 'S14'><span style="white-space: pre;"><span>                       </span><span style="color: rgb(160, 32, 240);">'OptimizeHyperparameters'</span><span>,{</span><span style="color: rgb(160, 32, 240);">'Lambda'</span><span>},HyperparameterOptimizationOptions',opts)</span></span></div></div><h2  class = 'S11' id = 'H_682A76AA' ><span style=' font-weight: bold;'>Use K-fold cross-validation with the Regression or Classification Learner Apps</span></h2><div  class = 'S2'><span>When training the 8th degree polynomial regression model without regularization and cross-validation, you most likely found that the resulting models looked very different after successive trainings which was a result of the random nature of the partition selection and small dataset size. By using K-fold cross validation, we can achieve more consistent results- even before adding regularization! K-fold cross-validation can therefore be helpful when dealing with high variance due to small data set size. Run the code below to reload the data and create the 8th degree polynomial features, then follow the instructions to train a polynomial regression model using K-fold cross-validation using the Regression Learner App. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>load </span><span style="color: rgb(160, 32, 240);">ex5data1.mat</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>LevelChange = [X;Xtest;Xval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Outflow = [y;ytest;yval];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>clearvars </span><span style="color: rgb(160, 32, 240);">-except LevelChange Outflow</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>mu = mean(LevelChange);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>sd = std(LevelChange);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>LevelChange = ((LevelChange-mu)/sd).^(1:8);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>data = [LevelChange,Outflow];   </span></span></div></div></div><div  class = 'S2' id = 'H_0E951879' ><span style=' font-weight: bold;'>Note: </span><span>If you have difficulty reading the instructions below while the app is open in MATLAB Online, export this script to a pdf file which you can then use to display the instructions in a separate browser tab or window. To export this script, click on the 'Save' button in the 'Live Editor' tab above, then select 'Export to PDF'.</span></div><h3  class = 'S15' id = 'H_FE75A9EF' ><span>Open the app and select the variables</span></h3><ol  class = 'S3'><li  class = 'S4'><span>In the MATLAB Apps tab, select the </span><span style=' font-weight: bold;'>Regression Learner</span><span> app from the Machine Learning section (you may need to expand the menu of available apps).</span></li><li  class = 'S4'><span>Select '</span><span style=' font-weight: bold;'>New Session -&gt; From Workspace</span><span>' to start a new interactive session.</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Workspace Variable'</span><span>, select '</span><span style=' font-weight: bold;'>data</span><span>'.</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Validation</span><span>' select '</span><span style=' font-weight: bold;'>Cross-Validation</span><span>' (if not already selected).</span></li><li  class = 'S4'><span>Use the slider bar to select 10 folds.</span></li><li  class = 'S4'><span>Click the '</span><span style=' font-weight: bold;'>Start Session</span><span>' button.</span></li></ol><h3  class = 'S16' id = 'H_150C2B14' ><span>Select and train the model</span></h3><ol  class = 'S3'><li  class = 'S4'><span>In the Model Type list, the default model is '</span><span style=' font-weight: bold;'>Fine Tree</span><span>'. Expand the model list and select '</span><span style=' font-weight: bold;'>Linear</span><span>' from the '</span><span style=' font-weight: bold;'>Linear Regression Models</span><span>' list.</span></li><li  class = 'S4'><span>Select '</span><span style=' font-weight: bold;'>Train</span><span>' to train the model.</span></li></ol><h3  class = 'S16' id = 'H_07954395' ><span>Export and visualize the model</span></h3><ol  class = 'S3'><li  class = 'S4'><span>To plot the results vs. the original level change variable in the app, select '</span><span style=' font-weight: bold;'>column_1</span><span>' under '</span><span style=' font-weight: bold;'>X-axis</span><span> in the</span><span style=' font-weight: bold;'> 'Response Plot'.</span></li><li  class = 'S4'><span>Next, select '</span><span style=' font-weight: bold;'>Export Model -&gt; Export Model</span><span>'.</span></li><li  class = 'S4'><span>Select the default output variable name ('</span><span style=' font-weight: bold;'>trainedModel</span><span>').</span></li><li  class = 'S4'><span>Extract the linear model variable and plot the model vs the dataset by running the code below</span></li></ol><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>polyRegMdl = trainedModel.LinearModel</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xplot = linspace(min(LevelChange(:,1)),max(LevelChange(:,1)))';</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Y = predict(polyRegMdl,xplot.^(1:8));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(LevelChange(:,1)*sd+mu,Outflow,</span><span style="color: rgb(160, 32, 240);">'bo'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>,10);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xplot(:,1)*sd+mu,Y,</span><span style="color: rgb(160, 32, 240);">'--'</span><span>,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>,2); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylim([min(Outflow) max(Outflow)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>legend({</span><span style="color: rgb(160, 32, 240);">'Data'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Model'</span><span>},</span><span style="color: rgb(160, 32, 240);">'Location'</span><span>,</span><span style="color: rgb(160, 32, 240);">'northwest'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Change in water level (x)'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Water flowing out of the dam (y)'</span><span>);</span></span></div></div></div><div  class = 'S12'><span>Note that there is no distinction between training and validation data in the plot above- all data values are used in training and validation with K-fold cross-validation. Compare the plot above to the original unregularized 8th degree model from the first section: </span></div><ul  class = 'S3'><li  class = 'S4'><span>Which method performs better? </span></li><li  class = 'S4'><span>Which is more consistent after multiple trainings? </span></li><li  class = 'S4'><span>How does the current K-fold validated model compare to the 4th order, regularized model developed in the second section of this script?</span></li></ul></div>
<br>
<!-- 
##### SOURCE BEGIN #####
%% MATLAB Companion Script for _Machine Learning_ ex5 (Optional)
%% Introduction
% Coursera's _Machine Learning_ was designed to provide you with a greater understanding 
% of machine learning algorithms- what they are, how they work, and where to apply 
% them. You are also shown techniques to improve their performance and to address 
% common issues. As is mentioned in the course, there are many tools available 
% that allow you to use machine learning algorithms _without_ having to implement 
% them yourself. This Live Script was created by MathWorks to help _Machine Learning_ 
% students explore the data analysis and machine learning tools available in MATLAB.
%% FAQ
% *Who is this intended for?*
%% 
% * This script is intended for students using MATLAB Online who have completed 
% ex5 and want to learn more about the corresponding machine learning tools in 
% MATLAB.
%% 
% *How do I use this script?*
%% 
% * In the sections that follow, read the information provided about the data 
% analysis and machine learning tools in MATLAB, then run the code in each section 
% and examine the results. You may also be presented with instructions for using 
% a MATLAB machine learning app. This script should be located in the ex5 folder 
% which should be set as your Current Folder in MATLAB Online.
%% 
% *Can I use the tools in this companion script to complete the programming 
% exercises?*
%% 
% * No. Most algorithm steps implemented in the programming exercises are handled 
% automatically by MATLAB machine learning functions. Additionally, the final 
% results will be similar, but not identical, to those in the programming exercises 
% due to differences in implementation, parameter settings, and randomization.
%% 
% *Where can I obtain help with this script or report issues?*
%% 
% * As this script is not part of the original course materials, please direct 
% any questions, comments, or issues to the _MATLAB Help_ discussion forum.
%% Regularized Linear Regression and Cross-Validation
% In this Live Script, we will use functions and apps from the <https://www.mathworks.com/products/statistics.html 
% Statistics and Machine Learning Toolbox> to partition data into training and 
% validation sets and determine optimal regularization strengths using cross-validation.
%% Files needed for this script
%% 
% * |ex5data1.mat| - Waterflow dataset
%% Data Partitioning and High Bias and High Variance Models
% Many functions used to train models in the Statistics and Machine Learning 
% Toolbox are equipped to partition a dataset into training and cross-validation 
% sets automatically. However, creating a fixed partition is often helpful for 
% reproducibility of results, or when comparing the performance of several different 
% algorithms. These partitions can also be useful when training models using functions 
% for building and training simpler models like |fitlm| and |fitglm| which do 
% not partition data automatically or provide options for validation. In this 
% section we will use the |cvpartition| function to partition the water flow data 
% from ex5.
%% Load the data
% Recall that the data contained in |ex5data1.mat| is divided into three sets:
%% 
% * A *training* set to train the model
% * A *cross-validation* set to determine the regularization parameter
% * A *test* set to evaluate performance. 
%% 
% In MATLAB, there is no need to store training and validation sets in separate 
% variables. Run the code below to combine the training, test, and validation 
% sets into a single |table| variable, |data|, and clear the original variables.

clear;
load ex5data1.mat;
LevelChange = [X;Xtest;Xval];
Outflow = [y;ytest;yval];
data = table(LevelChange,Outflow);
clearvars -except data;
%% Partition the dataset
% Next we will use the <https://www.mathworks.com/help/stats/cvpartition.html 
% |cvpartition|> function to partition the water flow data into a training set 
% with 12 examples (for direct comparison with your results from ex5) and validation 
% set with the remaining data. The result will be a <https://www.mathworks.com/help/stats/cvpartition-class.html 
% |cvpartition|> variable that contains the partition information and the indicies 
% of corresponding to the two sets. This variable can then be passed to MATLAB 
% machine learning functions along with the original dataset for model training 
% and validation. Alternatively, we can also extract a binary vector corresponding 
% to the training and validation examples from the partition variable using the 
% |training| and |test| functions, as demonstrated below, for use with functions 
% like |fitlm| and |fitglm|. 

m = height(data); % height() returns the number of rows in a table
c = cvpartition(m,'HoldOut',m-12) % Choose 12 training examples
tinds = training(c) % Training indices
%% 
% *Note*: In the terminology of the Statistics and Machine Learning Toolbox, 
% 'test' sets correspond to the 'validation' sets described in class. A 'test' 
% set as described in the course - data not used in training OR validation - would 
% still need to be stored in a separate |table| or matrix.
%% Fit a high bias and high variance model to the training data
% As in ex5, we will fit a linear model as an example of a high bias model, 
% followed by an 8th degree polynomial model as a high variance model example. 
% The code below uses |fitlm| to train the models using then plots the results. 
% If you are interested, you can re-run both this section and the previous one 
% to visualize the effect that the small training set size and random partition 
% selections have on high bias/variance models. 

% Train the models
highBiasMdl = fitlm(data(tinds,[1,2]));
HighVarMdl = fitlm(data(tinds,[1 2]),'poly8');
% Plot the model and training data
xplot = linspace(min(data.LevelChange(tinds)),max(data.LevelChange(tinds)))';
y1 = predict(highBiasMdl,xplot);
y2 = predict(HighVarMdl,xplot);
figure; hold on;
plot(data.LevelChange(tinds),data.Outflow(tinds),'rx','MarkerSize',10,'LineWidth',2);
plot(xplot,y1); plot(xplot,y2); hold off;
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
legend({'Training Data','Linear Model','Polynomial Model'})
%% Regularized Linear Regression using |fitrlinear|
% As discussed in ex5, the linear model is too simple to accurately reflect 
% the complexity of the water flow data, while polynomial model tends to overfit 
% the data and changes dramatically depending on the partition. In ex5 you found 
% that the performance of the polynomial model can be improved on new data by 
% adding regularization during training. Cross validation was then used to determine 
% the 'best' value of the regularization parameter, $\lambda$. In this section, 
% we use the <https://www.mathworks.com/help/stats/fitrlinear.html |fitrlinear|> 
% function to fit a regularized polynomial model to the data. Like |fitclinear| 
% (used in the companion script for ex2), |fitrlinear| is designed to train models 
% with a large number of variables and it also includes options for adding regularization. 
%% Load and normalize the data
% The code below will reload the data and create a (normalized) polynomial feature 
% matrix |X| by combining the training and validation sets as before. As with 
% |fitclinear|, |fitrlinear| requires feature and response data as numeric arrays 
% (i.e. not in a |table|). Since we found that the 8th degree polynomial model 
% was subject to very high variance in the previous section, we also limit the 
% polynomial degree to 4 to outline a more realistic model example.

load ex5data1.mat;
LevelChange = [X;Xtest;Xval];
Outflow = [y;ytest;yval];
clearvars -except LevelChange Outflow;
mu = mean(LevelChange);
sd = std(LevelChange);
LevelChange = ((LevelChange-mu)/sd).^(1:4);
%% Partition the data
% In this section we will let |fitrlinear| partition the data automatically 
% for us. We also provide code in this section and the next one to create the 
% corresponding model using a |cvpartition| variable as comments.  Run the code 
% below to select the proportion of data to hold out during training for validation 
% purposes. 

holdout = 0.35;
% c = cvpartiton(length(LevelChange),'HoldOut',holdout)
%% Fit linear regression models using multiple $\lambda$ values
% In the companion script for ex3, we trained a regularized model by passing 
% a single regularization value |lambda| to |fitclinear| using the |'Lambda'| 
% option. We can also use this option with |fitrlinear|. However, by passing _a 
% vector_ of $\lambda$ values to these functions, a separate model will automatically 
% be trained for each $\lambda$ value. Run the code below to create a vector of 
% $\lambda$ values and train the regression models for each $\lambda$. When using 
% multiple regularization values with |fitrlinear|, the trained models will be 
% returned in a <https://www.mathworks.com/help/stats/regressionpartitionedlinear-class.html 
% |RegressionPartitionedLinear|> model variable instead of a <https://www.mathworks.com/help/stats/regressionlinear-class.html 
% |RegressionLinear|> variable which is discussed in the next section.

lambda = [0 0.01 0.03 0.1 0.3 1];
partMdl = fitrlinear(LevelChange,Outflow,'Learner','leastsquares','Regularization','Ridge','Lambda',lambda,'HoldOut',holdout)
% partMdl = fitrlinear(LevelChange,Outflow,'Learner','leastsquares','Regularization','Ridge','Lambda',lambda,'CVPartition',c)
%% Working with partitioned model variables
% The individual regression models are contained in the |Trained| property of 
% a |RegressionPartitionedLinear| variable. In the case of hold-out cross validation, 
% the |Trained| property is a cell array which contains a single |RegressionLinear| 
% (or |ClassificationLinear| when using |fitclinear|) model variable. This variable 
% contains the trained models for each regularization value $\lambda$. The model 
% partition that was used during training and validation is found in the |Partition| 
% property. 
% 
% The model coefficients ($\theta$) are found in the $p\times l$  matrix |Beta| 
% and the $1\times l$ vector |bias|, where $p$ is the number of model coefficients 
% (4 in the current example) and $l$ is the number of $\lambda$ values (6). The 
% columns of |Beta| therefore correspond to coefficients for a particular model. 
% The model coefficients and the corresponding lambda values are displayed below 
% for comparison. Run the code below to extract the trained model variable, the 
% partition variable, and the model coefficients. Do the increasing regularization 
% values have the expected effect on the size of the coefficients and bias?

polyRegMdl = partMdl.Trained{1}
c = partMdl.Partition
[polyRegMdl.Bias; polyRegMdl.Beta]
lambda
%% Evaluate model performance and select models using |kfoldLoss| and |selectModels|
% Instead of using learning curves to detect high bias or variance issues, followed 
% validation curves to determine the optimal regularization parameter value, we 
% will use the <https://www.mathworks.com/help/stats/regressionpartitionedlinear.kfoldloss.html 
% |kfoldLoss|>|*| function to compute the cost of all models in the partitioned 
% model variable using the validation set. We then extract the individual model 
% with the lowest cost using the <https://www.mathworks.com/help/stats/regressionlinear.selectmodels.html 
% |selectModels|> function. The result will be a |RegressionLinear| model variable 
% corresponding to the best model as found by cross-validation.

valCost = kfoldLoss(partMdl)
[~,bestidx] = min(valCost)
fprintf('The best value of lambda found by cross-validation is: %g', lambda(bestidx))
bestRegMdl = selectModels(polyRegMdl,bestidx)
%% 
% *K-fold cross validation is discussed at the end of this Live Script. K-fold 
% validation with 1 fold is equivalent to hold-out validation.
%% Predict the response for all models and visualize the results
% Run the code below to plot the training data, validation data, and the models 
% for each $\lambda$. Note, when using the |predict| function along with a model 
% variable containing multiple models (coefficient sets), |predict| will return 
% a response _matrix_, where the $i$th column of the matrix corresponds to the 
% prediction using the _i_th model (trained with the $i$th regularization value).

% Extract the training and validation data
xtrain = LevelChange(training(c),1); 
ytrain = Outflow(training(c));
xval = LevelChange(test(c),1); 
yval = Outflow(test(c));
% Plot the data and models
xplot = linspace(min(LevelChange(:,1)),max(LevelChange(:,1)))';
xplot = xplot.^(1:4);
Y = predict(polyRegMdl,xplot);
figure; hold on;
plot(xtrain*sd+mu,ytrain,'go','MarkerSize',10);
plot(xval*sd+mu,yval,'LineStyle','none','Marker','square','MarkerSize',10);
plt = plot(xplot(:,1)*sd+mu,Y,'REPLACE_WITH_DASH_DASH'); hold off;
set(plt,{'Color'},num2cell(jet(length(lambda)),2));
set(plt(bestidx),'LineStyle','-','LineWidth',2);
ylim([min(Outflow) max(Outflow)]);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
legend(['Training','Validation',cellstr("lambda = "+string(lambda))],'Location','NorthWest')
title(sprintf('Best lambda = %g',lambda(bestidx)));
%% 
% 
%% Hyperparameter Optimization
% Model parameters, such as the regularization strength or optimization solver 
% settings, are commonly referred to as _hyperparameters_. It is often the case 
% that we do not want to explore the effect of hyperparameters on a model; rather 
% we are only interested in obtaining _the best_ model and/or hyperparameter value 
% as judged by cross-validation. In these cases we can utilize the _hyperparameter 
% optimization_ options in |fitrlinear/fitclinear|, which then automatically return 
% the best model found after training over a set or range of hyperparameter settings. 
% In this section we use hyperparameter optimization with |fitrlinear| to automatically 
% obtain the best cross-validation strength and the corresponding model.
%% Specify the cross-validation method
% To use hyperparameter optimization, the fist step is to create a |structure| 
% variable that contains information about the cross-validation method that will 
% be used to judge model performance. We can then provide a list of parameters 
% to optimize using the |'OptimizeHyperparameters'| option when training the model 
% and providing a cell array of parameter names to be optimized. If desired, we 
% can also adjust the permissible range or values of hyperparameters to be searched 
% during optimization from their default values by obtaining the corresponding 
% <https://www.mathworks.com/help/stats/optimizablevariable.html |optimizableVariable|> 
% structure for the given machine learning function, method, and dataset by using 
% the <https://www.mathworks.com/help/stats/hyperparameters.html |hyperparameters|> 
% function. Below we obtain the variable for |fitrlinear| with least-squares regression 
% and the current dataset, then select only the first optimizable parameter, $\lambda$. 
% 
% Run the code below to create an options structure for hold-out cross-validation 
% using the same percentage as in the previous section, then create a hyperparameter 
% options variable with a custom range of $\lambda$ values to search.

opts = struct('HoldOut',holdout);
optimizableParams = hyperparameters('fitrlinear',LevelChange,Outflow)
optimizableParams = optimizableParams(1)
optimizableParams.Range = [1e-4, 1]
%% Automatically select $\lambda$ using hyperparameter optimization
% By including the validation options structure and the hyperparameter options 
% variable created in the previous section and naming the <https://www.mathworks.com/help/stats/hyperparameters.html 
% hyperparameter to be optimized> in the call to |fitrlinear| or |fitclinear|, 
% these functions will automatically search through the given parameter values 
% and return a model variable corresponding to the best $\lambda$ found as judged 
% by cross validation. Since many models (30 by default) are trained during the 
% search, this will take considerably longer to run than when training a single 
% model. During training, the progress is displayed. If desired, the fit information 
% for the final model, along with the hyperparameter optimization information, 
% can also be obtained by providing additional output variables. See the <https://www.mathworks.com/help/stats/fitrlinear.html#bu216n7_sep_shared-HyperparameterOptimizationResults 
% documentation> for details.
% 
% Run the code below to call |fitrlinear| with hyperparameter optimization for 
% the regularization strength. The final model variable will be displayed, along 
% with the best $\lambda$ value found (you may have to scroll past the progress 
% plots in the output). The best model found in the previous section using a fixed 
% set of parameters and manual model selection will be plotted alongside the optimized 
% model computed below for comparison. Which model performs better?

optimRegMdl = fitrlinear(LevelChange,Outflow,...
                       'Learner','leastsquares',...
                       'Regularization','Ridge',...
                       'OptimizeHyperparameters',optimizableParams,...
                       'HyperparameterOptimizationOptions',opts)
figure; hold on;
plot(xtrain*sd+mu,ytrain,'go','MarkerSize',10);
plot(xval*sd+mu,yval,'LineStyle','none','Marker','square','MarkerSize',10);
plt = plot(xplot(:,1)*sd+mu,Y(:,bestidx),'b','LineWidth',2); 
plot(xplot(:,1)*sd+mu,predict(optimRegMdl,xplot),'r','LineWidth',2); hold off;
ylim([min(Outflow) max(Outflow)]);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
legend('Training','Validation',sprintf('lambda = %g',lambda(bestidx)),'Optimized lambda','Location','NorthWest')
title(sprintf('Best lambda estimated by hyperparameter\noptimization: %g',optimRegMdl.Lambda));               
%% K-Fold Cross-Validation
% This section discusses the most common type of cross validation used with 
% MATLAB machine learning functions - K-fold cross-validation - which is not used 
% in the programming exercises. 
%% Introduction
% Recall that the cross-validation technique used in the programming exercises 
% is referred to as 'hold-out' validation, as the validation examples are 'held 
% out' of the training process. This method is most often used when there is a 
% large number of training examples relative to the number of parameters in the 
% model. For smaller datasets like the water flow example, the model coefficients 
% and optimal parameter values can depend strongly on how the training and validation 
% sets are distributed and the random nature with which they are selected. There 
% are several <https://www.mathworks.com/discovery/cross-validation.html alternative 
% cross-validation methods> available in MATLAB that are preferable in situations 
% where there is not much data available. The most common is referred to as _K-fold_ 
% cross-validation, 
% 
% The idea behind K-fold cross-validation is the following:
%% 
% # Start with a data set, $X$ and choose a number of 'folds', $k$
% # Randomly select $k$ equally sized subsets (folds) of $X$: $X_1,\ldots, X_k$
% # Denote $X_1$ as the cross-validation set: $X^{cv}_{1}$ and combine the other 
% subsets into a single training set, $X^{t}_{1}$ 
% # Train and cross-validate a model using $X^{t}_{1}$ and $X^{cv}_{1}$ to provide 
% a set of model coefficients, $\theta_1$.
% # Repeat steps 3 and 4 with $X_i$ as the chosen validation set for $i = 2,\ldots,k$, 
% until there are $k$ sets of trained coefficients, $\theta_1,\ldots,\theta_k$
% # Average the $k$ sets of coefficients to produce a final set of model coefficients, 
% $\theta$
%% Using K-fold cross-validation instead of hold-out validation
% In the following section we outline two ways to incorporate K-fold cross-validation 
% instead of hold-out cross-validation when using MATLAB machine learning functions. 
% Sample code is provided for each method, which can be used in the first part 
% of the script if you are interested in trying out K-fold cross validation for 
% yourself.
% *Create a K-fold |cvpartition| variable for use with |fitlm| or |fitglm|:*
% The code below will create a |cvpartition| variable for K-fold cross-validation:
%%
% 
%   cvpartition(m,'KFold',k); 
%
%% 
% * |m| is the number of examples in your data set
% * |k| is the desired number of folds
% *Use K-fold cross-validation with |fitrlinear| or |fitclinear*|
% Use the |'KFold'| option with the desired number of folds:
%%
% 
%   fitrlinear(LevelChange,Outflow,'Learner','leastsquares','Regularization','Ridge','Lambda',lambda,'KFold',k)
%
% *Use K-fold cross-validation and hyperparameter optimization with |fitrlinear| or |fitclinear*|
% Create an options structure with the |'KFold'| option and number of folds, 
% then pass it to a training function using the |'HyperparameterOptimizationOptions'| 
% option:
%%
% 
%   opts = struct('KFold',k);
%   optimizableParams = hyperparameters('fitrlinear',LevelChange,Outflow);
%   
%   optimRegMdl = fitrlinear(LevelChange,Outflow,'Learner','leastsquares','Regularization','Ridge',...
%                          'OptimizeHyperparameters',{'Lambda'},HyperparameterOptimizationOptions',opts)
%
%% *Use K-fold cross-validation with the Regression or Classification Learner Apps*
% When training the 8th degree polynomial regression model without regularization 
% and cross-validation, you most likely found that the resulting models looked 
% very different after successive trainings which was a result of the random nature 
% of the partition selection and small dataset size. By using K-fold cross validation, 
% we can achieve more consistent results- even before adding regularization! K-fold 
% cross-validation can therefore be helpful when dealing with high variance due 
% to small data set size. Run the code below to reload the data and create the 
% 8th degree polynomial features, then follow the instructions to train a polynomial 
% regression model using K-fold cross-validation using the Regression Learner 
% App. 

clear;
load ex5data1.mat;
LevelChange = [X;Xtest;Xval];
Outflow = [y;ytest;yval];
clearvars -except LevelChange Outflow;
mu = mean(LevelChange);
sd = std(LevelChange);
LevelChange = ((LevelChange-mu)/sd).^(1:8);
data = [LevelChange,Outflow];   
%% 
% *Note:* If you have difficulty reading the instructions below while the app 
% is open in MATLAB Online, export this script to a pdf file which you can then 
% use to display the instructions in a separate browser tab or window. To export 
% this script, click on the 'Save' button in the 'Live Editor' tab above, then 
% select 'Export to PDF'.
% Open the app and select the variables
%% 
% # In the MATLAB Apps tab, select the *Regression Learner* app from the Machine 
% Learning section (you may need to expand the menu of available apps).
% # Select '*New Session -> From Workspace*' to start a new interactive session.
% # Under '*Workspace Variable'*, select '*data*'.
% # Under '*Validation*' select '*Cross-Validation*' (if not already selected).
% # Use the slider bar to select 10 folds.
% # Click the '*Start Session*' button.
% Select and train the model
%% 
% # In the Model Type list, the default model is '*Fine Tree*'. Expand the model 
% list and select '*Linear*' from the '*Linear Regression Models*' list.
% # Select '*Train*' to train the model.
% Export and visualize the model
%% 
% # To plot the results vs. the original level change variable in the app, select 
% '*column_1*' under '*X-axis* in the *'Response Plot'.*
% # Next, select '*Export Model -> Export Model*'.
% # Select the default output variable name ('*trainedModel*').
% # Extract the linear model variable and plot the model vs the dataset by running 
% the code below

polyRegMdl = trainedModel.LinearModel
xplot = linspace(min(LevelChange(:,1)),max(LevelChange(:,1)))';
Y = predict(polyRegMdl,xplot.^(1:8));
figure; hold on;
plot(LevelChange(:,1)*sd+mu,Outflow,'bo','MarkerSize',10);
plot(xplot(:,1)*sd+mu,Y,'REPLACE_WITH_DASH_DASH','LineWidth',2); hold off;
ylim([min(Outflow) max(Outflow)]);
legend({'Data','Model'},'Location','northwest')
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
%% 
% Note that there is no distinction between training and validation data in 
% the plot above- all data values are used in training and validation with K-fold 
% cross-validation. Compare the plot above to the original unregularized 8th degree 
% model from the first section: 
%% 
% * Which method performs better? 
% * Which is more consistent after multiple trainings? 
% * How does the current K-fold validated model compare to the 4th order, regularized 
% model developed in the second section of this script?
##### SOURCE END #####
--></body></html>