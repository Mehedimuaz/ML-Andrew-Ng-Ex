
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,IE=9,chrome=1"><meta name="generator" content="MATLAB"><title>MATLAB Companion Script for Machine Learning ex2 (Optional)</title><style type="text/css">.rtcContent { padding: 30px; } .S0 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 28.8px; min-height: 0px; white-space: pre-wrap; color: rgb(213, 80, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 24px; font-weight: 400; text-align: left;  }
.S1 { margin: 20px 10px 5px 4px; padding: 0px; line-height: 20px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 20px; font-weight: 700; text-align: left;  }
.S2 { margin: 2px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: left;  }
.S3 { margin: 10px 0px 20px; padding-left: 0px; font-family: Helvetica, Arial, sans-serif; font-size: 14px;  }
.S4 { margin-left: 56px; line-height: 21px; min-height: 0px; text-align: left; white-space: pre-wrap;  }
.S5 { margin-bottom: 20px; padding-bottom: 4px;  }
.S6 { margin: 0px; padding: 10px 0px 10px 5px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 700; text-align: start;  }
.S7 { margin: -1px 0px 0px; padding: 10px 0px 10px 7px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: start;  }
.CodeBlock { background-color: #F7F7F7; margin: 10px 0 10px 0;}
.S8 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 1px solid rgb(233, 233, 233); border-bottom: 0px none rgb(0, 0, 0); border-radius: 4px 4px 0px 0px; padding: 6px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S9 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 0px none rgb(0, 0, 0); border-radius: 0px; padding: 0px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S10 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 0px 0px 4px 4px; padding: 0px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S11 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 20px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 20px; font-weight: 700; text-align: left;  }
.S12 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 1px solid rgb(233, 233, 233); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 4px; padding: 6px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S13 { margin: 10px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: left;  }
.S14 { margin: 2px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: 400; text-align: center;  }
.S15 { margin: 15px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 17px; font-weight: 700; text-align: left;  }
.S16 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 17px; font-weight: 700; text-align: left;  }</style></head><body><div class = rtcContent><h1  class = 'S0' id = 'T_13370202' ><span>MATLAB Companion Script for </span><span style=' font-style: italic;'>Machine Learning</span><span> ex2 (Optional)</span></h1><h2  class = 'S1' id = 'H_5E50F655' ><span>Introduction</span></h2><div  class = 'S2'><span>Coursera's</span><span style=' font-style: italic;'> Machine Learning</span><span> was designed to provide you with a greater understanding of machine learning algorithms- what they are, how they work, and where to apply them. You are also shown techniques to improve their performance and to address common issues. As is mentioned in the course, there are many tools available that allow you to use machine learning algorithms </span><span style=' font-style: italic;'>without</span><span> having to implement them yourself. This Live Script was created by MathWorks to help </span><span style=' font-style: italic;'>Machine Learning</span><span> students explore the data analysis and machine learning tools available in MATLAB.</span></div><h2  class = 'S1' id = 'H_8D135D31' ><span>FAQ</span></h2><div  class = 'S2'><span style=' font-weight: bold;'>Who is this intended for?</span></div><ul  class = 'S3'><li  class = 'S4'><span>This script is intended for students using MATLAB Online who have completed ex2 and want to learn more about the corresponding machine learning tools in MATLAB.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>How do I use this script?</span></div><ul  class = 'S3'><li  class = 'S4'><span>In the sections that follow, read the information provided about the data analysis and machine learning tools in MATLAB, then run the code in each section and examine the results. You may also be presented with instructions for using a MATLAB machine learning app. This script should be located in the ex2 folder which should be set as your Current Folder in MATLAB Online.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>Can I use the tools in this companion script to complete the programming exercises?</span></div><ul  class = 'S3'><li  class = 'S4'><span>No. Most algorithm steps implemented in the programming exercises are handled automatically by MATLAB machine learning functions. Additionally, the results will be similar, but not identical, to those in the programming exercises due to differences in implementation, parameter settings, and randomization.</span></li></ul><div  class = 'S2'><span style=' font-weight: bold;'>Where can I obtain help with this script or report issues?</span></div><ul  class = 'S3'><li  class = 'S4'><span>As this script is not part of the original course materials, please direct any questions, comments, or issues to the </span><span style=' font-style: italic;'>MATLAB Help</span><span> discussion forum.</span></li></ul><h1  class = 'S0' id = 'T_20DD7EC2' ><span>Logistic Regression</span></h1><div  class = 'S2'><span>In this Live Script, logistic regression models are implemented using the </span><span style=' font-family: monospace;'>fitglm</span><span> and </span><span style=' font-family: monospace;'>fitclinear</span><span> functions from the </span><a href = "https://www.mathworks.com/products/statistics.html"><span>Statistics and Machine Learning Toolbox</span></a><span>. A quick tutorial is also included on the </span><span style=' font-style: italic;'>Classification Learner App</span><span>, which provides a graphical interface for creating classification models. </span></div><h2  class = 'S1' id = 'H_1CAA67AC' ><span>Files needed for this script</span></h2><ul  class = 'S3'><li  class = 'S4'><span>ex2data1.txt - Training set for logistic regression with one variable</span></li><li  class = 'S4'><span>ex2data2.txt</span><span style=' font-family: monospace;'> </span><span>- Training set for logistic regression with polynomial features</span></li></ul><div  class = 'S5'><div  class = 'S6'><span style=' font-weight: bold;'>Table of Contents</span></div><div  class = 'S7'><a href = "#T_13370202"><span>MATLAB Companion Script for Machine Learning ex2 (Optional)
</span></a><span>    </span><a href = "#H_5E50F655"><span>Introduction
</span></a><span>    </span><a href = "#H_8D135D31"><span>FAQ
</span></a><a href = "#T_20DD7EC2"><span>Logistic Regression
</span></a><span>    </span><a href = "#H_1CAA67AC"><span>Files needed for this script
</span></a><a href = "#T_34219452"><span>Logistic Regression with Two Variables
</span></a><span>    </span><a href = "#H_226C27C0"><span>Load the data into a table and preview the data
</span></a><span>    </span><a href = "#H_6812FCBB"><span>Train the model using fitglm
</span></a><span>    </span><a href = "#H_15CAFBB4"><span>Predict the training accuracy and probability of admission
</span></a><span>    </span><a href = "#H_ADE328D2"><span>Visualize the decision boundary 
</span></a><span>    </span><a href = "#H_7CB33821"><span>Using the Classification Learner App
</span></a><span>        </span><a href = "#H_8412613D"><span>Load the data
</span></a><span>        </span><a href = "#H_E8F6B4CC"><span>Open the app and select the variables
</span></a><span>        </span><a href = "#H_96AA51F1"><span>Select and train a classifier model
</span></a><span>        </span><a href = "#H_41BD5C3F"><span>Evaluate the model
</span></a><span>        </span><a href = "#H_E6FDDF7B"><span>Export the model
</span></a><a href = "#T_D5A9D1B8"><span>Logistic Regression with Polynomial Features
</span></a><span>    </span><a href = "#H_3B874FB7"><span>Load the data
</span></a><span>    </span><a href = "#H_8C5E1006"><span>Fit a logistic regression model with polynomial features and interaction terms 
</span></a><span>    </span><a href = "#H_1BDF11AA"><span>Predict the test results and training accuracy
</span></a><span>    </span><a href = "#H_1EF9D256"><span>Visualize the model and decision boundary
</span></a><a href = "#T_A5168830"><span>Logistic Regression with Regularization
</span></a><span>    </span><a href = "#H_EA89E9CE"><span>Load the data
</span></a><span>    </span><a href = "#H_44F1257B"><span>Fit the model
</span></a><span>    </span><a href = "#H_791A5B21"><span>Predict classes using the regularized model and plot the decision boundary
</span></a><a href = "#T_2BA87AFD"><span>Local Functions:
</span></a><span>    </span><a href = "#H_BE8B6E28"><span>plotMdlData</span></a></div></div><h1  class = 'S0' id = 'T_34219452' ><span>Logistic Regression with Two Variables</span></h1><div  class = 'S2'><span>This section covers the MATLAB implementation of logistic regression in two dimensions corresponding to the first part of ex2. Recall that the file ex2data1.txt contains scores for two exams in addition to a binary variable which denotes whether students were admitted to a university. In this section we obtain a logistic regression model to predict admission probability using the </span><span style=' font-family: monospace;'>fitglm</span><span> function. </span></div><h2  class = 'S1' id = 'H_226C27C0' ><span>Load the data into a </span><span style=' font-family: monospace;'>table</span><span> and preview the data</span></h2><div  class = 'S2'><span>Run the code below to load the data into a </span><span style=' font-family: monospace;'>table</span><span>. The first two columns (variables) will contain the exam scores and the third column the admission labels which we convert to </span><span style=' font-family: monospace;'>logical</span><span> values. We also compute some summary statistics on the three variables. After the table is displayed used the sort and filter controls (see the ex1 companion script for more information on </span><span style=' font-family: monospace;'>table</span><span> variables) to view only the scores of students that were admitted (</span><span style=' font-family: monospace;'>Admitted =</span><span> 'true'</span><span style=' font-family: monospace;'>)</span><span> or denied. Are the results what you would expect?</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data = readtable(</span><span style="color: rgb(160, 32, 240);">'ex2data1.txt'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data.Properties.VariableNames = {</span><span style="color: rgb(160, 32, 240);">'Exam1'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Exam2'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Admitted'</span><span>};</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data.Admitted = logical(data.Admitted)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>summary(data)</span></span></div></div></div><h2  class = 'S11' id = 'H_6812FCBB' ><span>Train the model using </span><span style=' font-family: monospace;'>fitglm</span></h2><div  class = 'S2'><span>Logistic regression models fall under a larger class of linear models referred to as </span><span style=' font-style: italic;'>generalized linear models</span><span> in MATLAB. To train a generalized linear model, we use the </span><a href = "https://www.mathworks.com/help/stats/fitglm.html"><span style=' font-family: monospace;'>fitglm</span></a><span> function. Run the code in this section to train a logistic regression model on the exam data. The result is a </span><a href = "https://www.mathworks.com/help/stats/generalizedlinearmodel-class.html"><span style=' font-family: monospace;'>GeneralizedLinearModel</span></a><span> variable which contains all of the information about the model. Note that to obtain a </span><span style=' font-style: italic;'>logistic</span><span> regression model from </span><span style=' font-family: monospace;'>fitglm</span><span>, we set the </span><span style=' font-family: monospace;'>Distribution</span><span> parameter to </span><span style=' font-family: monospace;'>binomial</span><span> as in the code below: </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S12'><span style="white-space: pre;"><span>logMdl = fitglm(data,</span><span style="color: rgb(160, 32, 240);">'Distribution'</span><span>,</span><span style="color: rgb(160, 32, 240);">'binomial'</span><span>)</span></span></div></div></div><div  class = 'S13'><span></span></div><div  class = 'S2'><span>Note the form of the model displayed in the output above. This is short-hand for </span></div><div  class = 'S14'><span texencoding="\mathrm{logit}(Admitted) = 1*\theta_0+Exam1*\theta_1+Exam2*\theta_2" style="vertical-align:-6px"><img src="data:image/png;base64,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" width="322" height="19.5" /></span><span> </span></div><div  class = 'S2'><span>Since </span><span texencoding="\mathrm{logit}(x)" style="vertical-align:-5px"><img src="data:image/png;base64,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" width="49.5" height="17.5" /></span><span> is the</span><span style=' font-style: italic;'> inverse function</span><span> of </span><span texencoding="\mathrm{sigmoid}(x)" style="vertical-align:-5px"><img src="data:image/png;base64,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" width="70.5" height="17.5" /></span><span>, this model is equivalent to logistic regression model form for the probability of admission used in ex2: </span></div><div  class = 'S14'><span texencoding="Admitted = h_{\theta}(x) = \mathrm{sigmoid}(\theta^Tx)" style="vertical-align:-6px"><img src="data:image/png;base64,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" width="210.5" height="20.5" /></span><span>,</span></div><div  class = 'S2'><span>where </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">x</span><span> includes the two exam scores and a bias term. As with the linear regression models trained in the ex1 companion script by </span><span style=' font-family: monospace;'>fitlm</span><span>, a bias term is added automatically by </span><span style=' font-family: monospace;'>fitglm</span><span>.</span></div><h2  class = 'S11' id = 'H_15CAFBB4' ><span>Predict the training accuracy and probability of admission</span></h2><div  class = 'S2'><span>Recall that a prediction of admission corresponds to a predicted probability &gt; 0.5. Run the code below to extract the </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">θ</span><span> values from the trained model, predict the probability of admission, and compute the training accuracy. Compare with your results from ex2.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>theta = logMdl.Coefficients.Estimate</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Predict the probability for a student with scores of 45 and 85</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>prob = predict(logMdl,[45 85]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'For a student with scores 45 and 85, we predict an admission probability of %f\n\n'</span><span>, prob);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Compute the training accuracy</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Admitted = predict(logMdl,data) &gt; 0.5;</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'Train Accuracy: %f\n'</span><span>, mean(double(Admitted == data.Admitted)) * 100);</span></span></div></div></div><h2  class = 'S11' id = 'H_ADE328D2' ><span>Visualize the decision boundary </span></h2><div  class = 'S2'><span>Run the code below to create a grid of exam scores and recreate the decision boundary plot from ex2. A local function, </span><span style=' font-family: monospace;'>plotMdlData,</span><span> has been included at the end of this script to create the plot.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the positive and negative examples</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plotMdlData(data);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the decision boundary</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xvals = [min(data.Exam1), max(data.Exam1)];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>yvals = -(theta(1)+theta(2)*xvals)/theta(3);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(xvals,yvals); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylim([min(data.Exam2),max(data.Exam2)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Labels and Legend</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Exam 1 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Exam 2 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>legend(</span><span style="color: rgb(160, 32, 240);">'Admitted'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Not admitted'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Decision Boundary'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div></div><div  class = 'S2' id = 'H_260BE492' ><span style=' font-weight: bold;'>Note: </span><span>If you have difficulty reading the instructions below while the app is open in MATLAB Online, export this script to a pdf file which you can then use to display the instructions in a separate browser tab or window. To export this script, click on the 'Save' button in the 'Live Editor' tab above, then select 'Export to PDF'.</span></div><h2  class = 'S1' id = 'H_7CB33821' ><span>Using the Classification Learner App</span></h2><div  class = 'S2'><span>In this section we provide the steps to reproduce the results of the previous section using the </span><a href = "https://www.mathworks.com/products/statistics/classification-learner.html"><span style=' font-style: italic;'>Classification Learner App</span></a><span>. This app offers a graphical interface for building, training, and evaluating classification models.</span></div><h3  class = 'S15' id = 'H_8412613D' ><span>Load the data</span></h3><div  class = 'S2'><span>Run the code below to clear the workspace and reload the housing data. Then follow the instructions in the next few sections to create and train a logistic regression classifier using the app.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data = readtable(</span><span style="color: rgb(160, 32, 240);">'ex2data1.txt'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>data.Properties.VariableNames = {</span><span style="color: rgb(160, 32, 240);">'Exam1'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Exam2'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Admitted'</span><span>};</span></span></div></div></div><h3  class = 'S16' id = 'H_E8F6B4CC' ><span>Open the app and select the variables</span></h3><ol  class = 'S3'><li  class = 'S4'><span>In the MATLAB Apps tab, select the </span><span style=' font-weight: bold;'>Classification Learner</span><span> app from the Machine Learning section (you may need to expand the menu of available apps).</span></li><li  class = 'S4'><span>Select '</span><span style=' font-weight: bold;'>New Session -&gt; From Workspace</span><span>' to start a new interactive session.</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Workspace Variable'</span><span>, select '</span><span style=' font-weight: bold;'>data</span><span>' (if not already selected).</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Response</span><span>' select '</span><span style=' font-weight: bold;'>Admitted</span><span>' (if not already selected).</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Predictors</span><span>' select '</span><span style=' font-weight: bold;'>Exam1</span><span>' and '</span><span style=' font-weight: bold;'>Exam2</span><span>' (if already selected).</span></li><li  class = 'S4'><span>Under '</span><span style=' font-weight: bold;'>Validation</span><span>' select '</span><span style=' font-weight: bold;'>No Validation</span><span>'</span></li><li  class = 'S4'><span>Click the '</span><span style=' font-weight: bold;'>Start Session</span><span>' button.</span></li></ol><h3  class = 'S16' id = 'H_96AA51F1' ><span>Select and train a classifier model</span></h3><div  class = 'S2'><span>There are many available classification models to choose from. In the model list the default model is 'Fine Tree'. To reproduce the logistic regression model obtained in the previous section:</span></div><ol  class = 'S3'><li  class = 'S4'><span>Expand the model list and select '</span><span style=' font-weight: bold;'>Logistic Regression</span><span>' from the '</span><span style=' font-weight: bold;'>Logistic Regression Classifiers</span><span>' list.</span></li><li  class = 'S4'><span>Select '</span><span style=' font-weight: bold;'>Train</span><span>' to train the model.</span></li></ol><h3  class = 'S16' id = 'H_41BD5C3F' ><span>Evaluate the model</span></h3><div  class = 'S2'><span>After training there are several options available for evaluating the model's performance: </span></div><ul  class = 'S3'><li  class = 'S4'><span>The results, including training accuracy, prediction speed and training time for the selected model is shown in the '</span><span style=' font-weight: bold;'>Current Model</span><span>' pane. </span></li><li  class = 'S4'><span>The model predictions including the predicted class and misclassified data points are visualized in the '</span><span style=' font-weight: bold;'>Scatter Plot</span><span>'. You can view the data points vs. the different predictor variables by selecting the desired variables for each axis from the '</span><span style=' font-weight: bold;'>Predictors</span><span>' list. (Exam1 and Exam2 are selected by default as there are only two predictors).</span></li><li  class = 'S4'><span>The '</span><span style=' font-weight: bold;'>Confusion Matrix</span><span>', '</span><span style=' font-weight: bold;'>ROC Curve</span><span>', and '</span><span style=' font-weight: bold;'>Parallel Coordinates</span><span>' plots provide additional means of evaluating the model.</span></li></ul><h3  class = 'S16' id = 'H_E6FDDF7B' ><span>Export the model</span></h3><div  class = 'S2'><span>Export and extract the trained model to the MATLAB workspace by following the steps below: </span></div><ol  class = 'S3'><li  class = 'S4'><span>Select '</span><span style=' font-weight: bold;'>Export Model -&gt; Export Model</span><span>'.</span></li><li  class = 'S4'><span>Select the default output variable name ('trainedModel').</span></li><li  class = 'S4'><span>Extract the linear model from the output variable by running the command below:</span></li></ol><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S12'><span style="white-space: pre;"><span class="warning_squiggle_rteliveEditorB7EFF6F6">logMdl</span><span> = trainedModel.GeneralizedLinearModel</span></span></div></div></div><div  class = 'S13'><span>The </span><span style=' font-family: monospace;'>logMdl</span><span> variable now contains the logistic regression model which can be used in the same manner as the model previously created by </span><span style=' font-family: monospace;'>fitglm</span><span>.</span></div><h1  class = 'S0' id = 'T_D5A9D1B8' ><span>Logistic Regression with Polynomial Features</span></h1><div  class = 'S2'><span>In the second part of ex2, you implemented a regularized logistic regression classifier model to predict whether microchips pass quality assurance based on the scores from two tests. In this section we will obtain a corresponding model using </span><span style=' font-family: monospace;'>fitglm</span><span>.</span></div><h2  class = 'S1' id = 'H_3B874FB7' ><span>Load the data</span></h2><div  class = 'S2'><span>Run the code below to load the test data and results into a table with test score variables </span><span style=' font-family: monospace;'>Test1</span><span>, </span><span style=' font-family: monospace;'>Test2</span><span>, and the binary variable </span><span style=' font-family: monospace;'>Pass</span><span>. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data = readtable(</span><span style="color: rgb(160, 32, 240);">'ex2data2.txt'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data.Properties.VariableNames = {</span><span style="color: rgb(160, 32, 240);">'Test1'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Test2'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Pass'</span><span>};</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>data.Pass = logical(data.Pass)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>summary(data)</span></span></div></div></div><h2  class = 'S11' id = 'H_8C5E1006' ><span>Fit a logistic regression model with polynomial features and interaction terms </span></h2><div  class = 'S2'><span>Recall that the different pass/fail outcomes could not be well separated by a logistic regression classifier using only the existing features. Instead, polynomial features up to the sixth power including interaction terms were created to capture the increased complexity of the data. Below we use the </span><span style=' font-family: monospace;'>fitglm</span><span> function to fit a logistic regression model including these polynomial features and interaction terms. Unlike your implementation in ex2, we </span><span style=' font-style: italic;'>will not explicitly create these terms</span><span>. Instead we'll including an additional</span><span style=' font-style: italic;'> model specification</span><span> parameter in the call to </span><span style=' font-family: monospace;'>fitglm</span><span>- see the documentation for more information about </span><a href = "https://www.mathworks.com/help/stats/fitglm.html#bt0dgf8-modelspec"><span>model specifications</span></a><span>. </span></div><div  class = 'S2'><span>Run the code below to fit a logistic regression model using with polynomial features and note the form of the model returned.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S12'><span style="white-space: pre;"><span>logMdl = fitglm(data,</span><span style="color: rgb(160, 32, 240);">'poly66'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Distribution'</span><span>,</span><span style="color: rgb(160, 32, 240);">'binomial'</span><span>)</span></span></div></div></div><h2  class = 'S11' id = 'H_1BDF11AA' ><span>Predict the test results and training accuracy</span></h2><div  class = 'S2'><span>Next we use the </span><a href = "https://www.mathworks.com/help/stats/compactgeneralizedlinearmodel.predict.html"><span style=' font-family: monospace;'>predict</span></a><span> function compute the probability of passing for the training examples to obtain the training accuracy. Again it is assumed that a probability &gt; 0.5 corresponds to passing. As with the model training in the previous section, there is no need to map the original features to their polynomial counterparts for prediction as we can pass the original data directly to </span><span style=' font-family: monospace;'>predict</span><span>.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Compute accuracy on our training set</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Pass = predict(logMdl,data) &gt; 0.5;</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'Train Accuracy: %f\n'</span><span>, mean(Pass == data.Pass) * 100);</span></span></div></div></div><h2  class = 'S11' id = 'H_1EF9D256' ><span>Visualize the model and decision boundary</span></h2><div  class = 'S2'><span>Run the code in this section to plot the decision boundary and compare with your result from ex2 for </span><span texencoding="\lambda = 0" style="vertical-align:-5px"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEkAAAAjCAYAAADYHCfgAAADYklEQVRoQ+2YWaiNURiGn4MMSaYkU66QhAtCIbkhJVIKGVKEkLFkyBzJjZIxUwgpEaEIF4ZLd3KlpIQIyTwUvbvvP62zzv7PXv9un+3ftVbtm7PXWftbz3q/sY64ShKoK7kjbiBCChBBhBQhBRAI2BKVFCEFEAjYkkVJ2jsV2AG8BZYAzwJ+I09bWgBjgbnAB6Ad0Am4DNwEfhYzNguk4cB54AcwGNgM7AL+5olCE7Z0AXYCw4BlwGPb2x84ArwE1gLv/DNCIbW2H7gP3Aa2AX3sxz7XACQpZi+wHFgEHPcedwJwCTgLrAO+uncKhdQXmA/sMUlOAZYCs4H3NQBJYUIAngKzgOeezZ2BE8A0YA5wrhxIPofR5m61AKk9cMAe+SiwGvjuXUhi2WTecgVYAHxM9oQqqZYhDTFX6gesAfalKF/ecRVQ+JgEPKoEJLnfSuBbzt0tubzMnAlcTLFX3vHQvlPmluoKqxwltQLWA71TpJs3ZhuA3WbUGFchnqEDDOBQU5vu+KtcSG4mUMpUSRC6ulpQnBj6Dyn70mJLsey9FdBHKxRSg/OzKmkgcAoYCWyxQJflvtWGpNSvGLQ4I6RbbubOAklZQrWGUr9WOZCyAK3EXtV3KluU0bIoSTXTQuBTFndraZlBkO6ai13w64lK3KoZzlBnoFYqC6Sy3G0ccBrQy6hinQw8qBFIcjW1HVkgHbIWpRBvQ9ytl1Wr462A1A8etr9da4aXr/SRoSXAKGu5Ovj1VClIbhy6YX762xSkZrG+4Aq8WbUDt8xyU3toMan2RGGlsJqCpO/U50g5X6ynuQd0s2mAmtwEUlvr6UpNBP4HJLctUaarr3+8h03qqTvAPOB1CCRVoGr01Nwq+ClL/HFeZgWgqcAI+6g/yutKajuNR9RvvvIM7WiTgenAKmC/OyVIU1J3K8vVPSdu9sYOTuR7Bjhm8E4685k8gnJHJY26fCCBKBUp0DeYKRWDpLZjO7DRJKdD5WbJ6mkK0/BKh2mipxmM31nnDZbChLxhkMXWJ2agPOWgtSCqp174hvuQ3DikKO+6meuiM6ywlMpUVDaa5uWNkNnTBlCWlsupMdfD9rApwfW0hy6V3XJ61+qaFSEF8I6QIqQAAgFbopIipAACAVuikiKkAAIBW/4Bk2fAJIBXxhQAAAAASUVORK5CYII=" width="36.5" height="17.5" /></span><span>. The code below creates a grid of test scores, predicts the probability of passing for each pair, the uses </span><span style=' font-family: monospace;'>contour</span><span> to estimate the location of the decision boundary where </span><span texencoding="probability = 0.5" style="vertical-align:-5px"><img src="data:image/png;base64,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" width="108" height="17.5" /></span><span>.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the positive and negative examples</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plotMdlData(data);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the decision boundary</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xvals = linspace(min(data.Test1), max(data.Test1));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>yvals = linspace(min(data.Test1), max(data.Test2));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>[X, Y] = meshgrid(xvals,yvals);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>p = predict(logMdl,[X(:),Y(:)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>contour(X,Y,reshape(p,size(X)),[0.5,0.5]); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Labels and legend</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Test 1 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Test 2 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>legend(</span><span style="color: rgb(160, 32, 240);">'Pass'</span><span>, </span><span style="color: rgb(160, 32, 240);">'Fail'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Decision Boundary'</span><span>)</span></span></div></div></div><h1  class = 'S0' id = 'T_A5168830' ><span>Logistic Regression with Regularization</span></h1><div  class = 'S2'><span>In this section, we use the </span><a href = "https://www.mathworks.com/help/stats/fitclinear.html"><span style=' font-family: monospace;'>fitclinear</span></a><span> function train a logistic regression model </span><span style=' font-style: italic;'>including regularization</span><span>. As the </span><span style=' font-family: monospace;'>fitclinear</span><span> function is generally used with </span><span style=' font-style: italic;'>high dimensional data</span><span> (i.e. with lots of variables- like our polynomial feature model) where storing data in </span><span style=' font-family: monospace;'>table</span><span> variables makes less sense, it takes training data in form of numeric matrices instead. </span></div><h2  class = 'S1' id = 'H_EA89E9CE' ><span>Load the data</span></h2><div  class = 'S2'><span>Run the code below to load the data into the feature matrix </span><span style=' font-family: monospace;'>X</span><span> and response vector </span><span style=' font-family: monospace;'>y</span><span>. We then also create the polynomial feature matrix, </span><span style=' font-family: monospace;'>Xpoly</span><span>. </span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span>clear;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>X = load(</span><span style="color: rgb(160, 32, 240);">'ex2data2.txt'</span><span>);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>y = X(:,3);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>X(:,3) = [];</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Create the polynomial feature matrix up to power 6</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>powers = [nchoosek(0:6,2); fliplr(nchoosek(0:6,2));1 1;2 2;3 3]';</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>powers(:,sum(powers)&gt;6) = [];</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>Xpoly = (X(:,1).^powers(1,:)).*(X(:,2).^powers(2,:));</span></span></div></div></div><h2  class = 'S11' id = 'H_44F1257B' ><span>Fit the model</span></h2><div  class = 'S2'><span>Next, we train the model using </span><span style=' font-family: monospace;'>fitclinear</span><span> with the regularization type set to </span><span style=' font-family: monospace;'>ridge</span><span> (this is the type of regularization used in ex2) and the strength given by </span><span style=' font-family: monospace;'>lambda</span><span>. The result is a </span><a href = "https://www.mathworks.com/help/stats/classificationlinear-class.html"><span style=' font-family: monospace;'>ClassificationLinear</span></a><span> model variable which contains all of the information about the model. The model coefficients are found in the </span><span style=' font-family: monospace;'>Bias</span><span> and </span><span style=' font-family: monospace;'>Beta</span><span> properties of the model variable. Use the control select a value of </span><span style="font-family: STIXGeneral, STIXGeneral-webfont, serif; font-style: italic; font-weight: 400; color: rgb(0, 0, 0);">λ</span><span>, then examine the effect on the training accuracy and decision boundary from the results in the next two sections:</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Choose lambda and train the model</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>lambda = </span></span><span>0.001</span><span style="white-space: pre;"><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>logMdl = fitclinear(Xpoly,y,</span><span style="color: rgb(160, 32, 240);">'Lambda'</span><span>,lambda,</span><span style="color: rgb(160, 32, 240);">'Learner'</span><span>,</span><span style="color: rgb(160, 32, 240);">'logistic'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Regularization'</span><span>,</span><span style="color: rgb(160, 32, 240);">'ridge'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>logMdl.Bias</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>logMdl.Beta</span></span></div></div></div><h2  class = 'S1' id = 'H_791A5B21' ><span>Predict classes using the regularized model and plot the decision boundary</span></h2><div  class = 'S2'><span>The </span><a href = "https://www.mathworks.com/help/stats/classificationlinear.predict.html"><span style=' font-family: monospace;'>predict</span></a><span> function is used below to classify data using the </span><span style=' font-family: monospace;'>ClassificationLinear</span><span> model variable in the same manner as with </span><span style=' font-family: monospace;'>GeneralizedLinearModel</span><span> variables. However, the </span><span style=' font-family: monospace;'>predict</span><span> function will return a</span><span style=' font-style: italic;'> class label </span><span>instead of probability score. If needed, we obtain the probaibility scores by requesting a second output from </span><span style=' font-family: monospace;'>predict</span><span>. See the code below used to plot the decision boundary. When you are done, try re-training the model using a different value of </span><span style=' font-family: monospace;'>lambda</span><span> and examine the effects. Which model do you think will generalize the best?</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Obtain the class labels and compute the training accuracy</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Pass = predict(logMdl,Xpoly);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'Train Accuracy: %f\n'</span><span>, mean(Pass == y) * 100);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the positve and negative examples</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>figure; hold </span><span style="color: rgb(160, 32, 240);">on</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plotMdlData(array2table([X y],</span><span style="color: rgb(160, 32, 240);">'VariableNames'</span><span>,{</span><span style="color: rgb(160, 32, 240);">'Test1'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Test2'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Pass'</span><span>})); </span></span></div></div><div class="inlineWrapper"><div  class = 'S9'></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the decision boundary</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xvals = linspace(min(X(:,1)), max(X(:,1)));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>yvals = linspace(min(X(:,2)), max(X(:,2)));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>[Xgrid, Ygrid] = meshgrid(xvals,yvals);</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>Xpolygrid = (Xgrid(:).^powers(1,:)).*(Ygrid(:).^powers(2,:));</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>[~,Score] = predict(logMdl,Xpolygrid); </span><span style="color: rgb(60, 118, 61);">% Obtain the probability scores</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>contour(Xgrid,Ygrid,reshape(Score(:,2),size(Xgrid)),[0.5,0.5]); hold </span><span style="color: rgb(160, 32, 240);">off</span><span>;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Labels and legend</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>xlabel(</span><span style="color: rgb(160, 32, 240);">'Test 1 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>ylabel(</span><span style="color: rgb(160, 32, 240);">'Test 2 score'</span><span>)</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span>legend(</span><span style="color: rgb(160, 32, 240);">'Pass'</span><span>, </span><span style="color: rgb(160, 32, 240);">'Fail'</span><span>,</span><span style="color: rgb(160, 32, 240);">'Decision Boundary'</span><span>)</span></span></div></div></div><h1  class = 'S0' id = 'T_2BA87AFD' ><span>Local Functions:</span></h1><h2  class = 'S1' id = 'H_BE8B6E28' ><span style=' font-weight: bold; font-family: monospace;'>plotMdlData</span></h2><div  class = 'S2'><span style=' font-family: monospace;'>plotMdlData</span><span> is used to plot the positive and negative examples for the data sets for better comparison with the plots in ex2.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>[] = plotMdlData(data)</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Reproduce the plots from ex2 with positive and negative results for an input table</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Extract variable names from 3 column table</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>varNames = data.Properties.VariableNames;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% Plot the data with + for true and 0 for false examples</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>inds = data.(varNames{3}) == 1;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(data.(varNames{1})(inds), data.(varNames{2})(inds), </span><span style="color: rgb(160, 32, 240);">'k+'</span><span>,</span><span style="color: rgb(160, 32, 240);">'LineWidth'</span><span>, 2, </span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>, 7); </span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>inds = data.(varNames{3}) == 0;</span></span></div></div><div class="inlineWrapper"><div  class = 'S9'><span style="white-space: pre;"><span>plot(data.(varNames{1})(inds), data.(varNames{2})(inds), </span><span style="color: rgb(160, 32, 240);">'ko'</span><span>, </span><span style="color: rgb(160, 32, 240);">'MarkerFaceColor'</span><span>, </span><span style="color: rgb(160, 32, 240);">'y'</span><span>,</span><span style="color: rgb(160, 32, 240);">'MarkerSize'</span><span>, 7);</span></span></div></div><div class="inlineWrapper"><div  class = 'S10'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div></div></div>
<br>
<!-- 
##### SOURCE BEGIN #####
%% MATLAB Companion Script for _Machine Learning_ ex2 (Optional)
%% Introduction
% Coursera's _Machine Learning_ was designed to provide you with a greater understanding 
% of machine learning algorithms- what they are, how they work, and where to apply 
% them. You are also shown techniques to improve their performance and to address 
% common issues. As is mentioned in the course, there are many tools available 
% that allow you to use machine learning algorithms _without_ having to implement 
% them yourself. This Live Script was created by MathWorks to help _Machine Learning_ 
% students explore the data analysis and machine learning tools available in MATLAB.
%% FAQ
% *Who is this intended for?*
%% 
% * This script is intended for students using MATLAB Online who have completed 
% ex2 and want to learn more about the corresponding machine learning tools in 
% MATLAB.
%% 
% *How do I use this script?*
%% 
% * In the sections that follow, read the information provided about the data 
% analysis and machine learning tools in MATLAB, then run the code in each section 
% and examine the results. You may also be presented with instructions for using 
% a MATLAB machine learning app. This script should be located in the ex2 folder 
% which should be set as your Current Folder in MATLAB Online.
%% 
% *Can I use the tools in this companion script to complete the programming 
% exercises?*
%% 
% * No. Most algorithm steps implemented in the programming exercises are handled 
% automatically by MATLAB machine learning functions. Additionally, the results 
% will be similar, but not identical, to those in the programming exercises due 
% to differences in implementation, parameter settings, and randomization.
%% 
% *Where can I obtain help with this script or report issues?*
%% 
% * As this script is not part of the original course materials, please direct 
% any questions, comments, or issues to the _MATLAB Help_ discussion forum.
%% Logistic Regression
% In this Live Script, logistic regression models are implemented using the 
% |fitglm| and |fitclinear| functions from the <https://www.mathworks.com/products/statistics.html 
% Statistics and Machine Learning Toolbox>. A quick tutorial is also included 
% on the _Classification Learner App_, which provides a graphical interface for 
% creating classification models. 
%% Files needed for this script
%% 
% * ex2data1.txt - Training set for logistic regression with one variable
% * ex2data2.txt - Training set for logistic regression with polynomial features
%% Logistic Regression with Two Variables
% This section covers the MATLAB implementation of logistic regression in two 
% dimensions corresponding to the first part of ex2. Recall that the file ex2data1.txt 
% contains scores for two exams in addition to a binary variable which denotes 
% whether students were admitted to a university. In this section we obtain a 
% logistic regression model to predict admission probability using the |fitglm| 
% function. 
%% Load the data into a |table| and preview the data
% Run the code below to load the data into a |table|. The first two columns 
% (variables) will contain the exam scores and the third column the admission 
% labels which we convert to |logical| values. We also compute some summary statistics 
% on the three variables. After the table is displayed used the sort and filter 
% controls (see the ex1 companion script for more information on |table| variables) 
% to view only the scores of students that were admitted (|Admitted =| 'true'|)| 
% or denied. Are the results what you would expect?

clear;
data = readtable('ex2data1.txt');
data.Properties.VariableNames = {'Exam1','Exam2','Admitted'};
data.Admitted = logical(data.Admitted)
summary(data)
%% Train the model using |fitglm|
% Logistic regression models fall under a larger class of linear models referred 
% to as _generalized linear models_ in MATLAB. To train a generalized linear model, 
% we use the <https://www.mathworks.com/help/stats/fitglm.html |fitglm|> function. 
% Run the code in this section to train a logistic regression model on the exam 
% data. The result is a <https://www.mathworks.com/help/stats/generalizedlinearmodel-class.html 
% |GeneralizedLinearModel|> variable which contains all of the information about 
% the model. Note that to obtain a _logistic_ regression model from |fitglm|, 
% we set the |Distribution| parameter to |binomial| as in the code below: 

logMdl = fitglm(data,'Distribution','binomial')
%% 
% 
% 
% Note the form of the model displayed in the output above. This is short-hand 
% for 
% 
% $$\mathrm{logit}(Admitted) = 1*\theta_0+Exam1*\theta_1+Exam2*\theta_2$$ 
% 
% Since $\mathrm{logit}(x)$ is the _inverse function_ of $\mathrm{sigmoid}(x)$, 
% this model is equivalent to logistic regression model form for the probability 
% of admission used in ex2: 
% 
% $Admitted = h_{\theta}(x) = \mathrm{sigmoid}(\theta^Tx)$,
% 
% where $x$ includes the two exam scores and a bias term. As with the linear 
% regression models trained in the ex1 companion script by |fitlm|, a bias term 
% is added automatically by |fitglm|.
%% Predict the training accuracy and probability of admission
% Recall that a prediction of admission corresponds to a predicted probability 
% > 0.5. Run the code below to extract the $\theta$ values from the trained model, 
% predict the probability of admission, and compute the training accuracy. Compare 
% with your results from ex2.

theta = logMdl.Coefficients.Estimate
% Predict the probability for a student with scores of 45 and 85
prob = predict(logMdl,[45 85]);
fprintf('For a student with scores 45 and 85, we predict an admission probability of %f\n\n', prob);
% Compute the training accuracy
Admitted = predict(logMdl,data) > 0.5;
fprintf('Train Accuracy: %f\n', mean(double(Admitted == data.Admitted)) * 100);
%% Visualize the decision boundary 
% Run the code below to create a grid of exam scores and recreate the decision 
% boundary plot from ex2. A local function, |plotMdlData,| has been included at 
% the end of this script to create the plot.

figure; hold on;
% Plot the positive and negative examples
plotMdlData(data);

% Plot the decision boundary
xvals = [min(data.Exam1), max(data.Exam1)];
yvals = -(theta(1)+theta(2)*xvals)/theta(3);
plot(xvals,yvals); hold off;
ylim([min(data.Exam2),max(data.Exam2)]);

% Labels and Legend
xlabel('Exam 1 score')
ylabel('Exam 2 score')
legend('Admitted','Not admitted','Decision Boundary')
hold off;
%% 
% *Note:* If you have difficulty reading the instructions below while the app 
% is open in MATLAB Online, export this script to a pdf file which you can then 
% use to display the instructions in a separate browser tab or window. To export 
% this script, click on the 'Save' button in the 'Live Editor' tab above, then 
% select 'Export to PDF'.
%% Using the Classification Learner App
% In this section we provide the steps to reproduce the results of the previous 
% section using the <https://www.mathworks.com/products/statistics/classification-learner.html 
% _Classification Learner App_>. This app offers a graphical interface for building, 
% training, and evaluating classification models.
% Load the data
% Run the code below to clear the workspace and reload the housing data. Then 
% follow the instructions in the next few sections to create and train a logistic 
% regression classifier using the app.

clear;
data = readtable('ex2data1.txt');
data.Properties.VariableNames = {'Exam1','Exam2','Admitted'};
% Open the app and select the variables
%% 
% # In the MATLAB Apps tab, select the *Classification Learner* app from the 
% Machine Learning section (you may need to expand the menu of available apps).
% # Select '*New Session -> From Workspace*' to start a new interactive session.
% # Under '*Workspace Variable'*, select '*data*' (if not already selected).
% # Under '*Response*' select '*Admitted*' (if not already selected).
% # Under '*Predictors*' select '*Exam1*' and '*Exam2*' (if already selected).
% # Under '*Validation*' select '*No Validation*'
% # Click the '*Start Session*' button.
% Select and train a classifier model
% There are many available classification models to choose from. In the model 
% list the default model is 'Fine Tree'. To reproduce the logistic regression 
% model obtained in the previous section:
%% 
% # Expand the model list and select '*Logistic Regression*' from the '*Logistic 
% Regression Classifiers*' list.
% # Select '*Train*' to train the model.
% Evaluate the model
% After training there are several options available for evaluating the model's 
% performance: 
%% 
% * The results, including training accuracy, prediction speed and training 
% time for the selected model is shown in the '*Current Model*' pane. 
% * The model predictions including the predicted class and misclassified data 
% points are visualized in the '*Scatter Plot*'. You can view the data points 
% vs. the different predictor variables by selecting the desired variables for 
% each axis from the '*Predictors*' list. (Exam1 and Exam2 are selected by default 
% as there are only two predictors).
% * The '*Confusion Matrix*', '*ROC Curve*', and '*Parallel Coordinates*' plots 
% provide additional means of evaluating the model.
% Export the model
% Export and extract the trained model to the MATLAB workspace by following 
% the steps below: 
%% 
% # Select '*Export Model -> Export Model*'.
% # Select the default output variable name ('trainedModel').
% # Extract the linear model from the output variable by running the command 
% below:

logMdl = trainedModel.GeneralizedLinearModel
%% 
% The |logMdl| variable now contains the logistic regression model which can 
% be used in the same manner as the model previously created by |fitglm|.
%% Logistic Regression with Polynomial Features
% In the second part of ex2, you implemented a regularized logistic regression 
% classifier model to predict whether microchips pass quality assurance based 
% on the scores from two tests. In this section we will obtain a corresponding 
% model using |fitglm|.
%% Load the data
% Run the code below to load the test data and results into a table with test 
% score variables |Test1|, |Test2|, and the binary variable |Pass|. 

clear;
data = readtable('ex2data2.txt');
data.Properties.VariableNames = {'Test1','Test2','Pass'};
data.Pass = logical(data.Pass)
summary(data)
%% Fit a logistic regression model with polynomial features and interaction terms 
% Recall that the different pass/fail outcomes could not be well separated by 
% a logistic regression classifier using only the existing features. Instead, 
% polynomial features up to the sixth power including interaction terms were created 
% to capture the increased complexity of the data. Below we use the |fitglm| function 
% to fit a logistic regression model including these polynomial features and interaction 
% terms. Unlike your implementation in ex2, we _will not explicitly create these 
% terms_. Instead we'll including an additional _model specification_ parameter 
% in the call to |fitglm|- see the documentation for more information about <https://www.mathworks.com/help/stats/fitglm.html#bt0dgf8-modelspec 
% model specifications>. 
% 
% Run the code below to fit a logistic regression model using with polynomial 
% features and note the form of the model returned.

logMdl = fitglm(data,'poly66','Distribution','binomial')
%% Predict the test results and training accuracy
% Next we use the <https://www.mathworks.com/help/stats/compactgeneralizedlinearmodel.predict.html 
% |predict|> function compute the probability of passing for the training examples 
% to obtain the training accuracy. Again it is assumed that a probability > 0.5 
% corresponds to passing. As with the model training in the previous section, 
% there is no need to map the original features to their polynomial counterparts 
% for prediction as we can pass the original data directly to |predict|.

% Compute accuracy on our training set
Pass = predict(logMdl,data) > 0.5;
fprintf('Train Accuracy: %f\n', mean(Pass == data.Pass) * 100);
%% Visualize the model and decision boundary
% Run the code in this section to plot the decision boundary and compare with 
% your result from ex2 for $\lambda = 0$. The code below creates a grid of test 
% scores, predicts the probability of passing for each pair, the uses |contour| 
% to estimate the location of the decision boundary where $probability = 0.5$.

figure; hold on;
% Plot the positive and negative examples
plotMdlData(data);

% Plot the decision boundary
xvals = linspace(min(data.Test1), max(data.Test1));
yvals = linspace(min(data.Test1), max(data.Test2));
[X, Y] = meshgrid(xvals,yvals);
p = predict(logMdl,[X(:),Y(:)]);
contour(X,Y,reshape(p,size(X)),[0.5,0.5]); hold off;
% Labels and legend
xlabel('Test 1 score')
ylabel('Test 2 score')
legend('Pass', 'Fail','Decision Boundary')
%% Logistic Regression with Regularization
% In this section, we use the <https://www.mathworks.com/help/stats/fitclinear.html 
% |fitclinear|> function train a logistic regression model _including regularization_. 
% As the |fitclinear| function is generally used with _high dimensional data_ 
% (i.e. with lots of variables- like our polynomial feature model) where storing 
% data in |table| variables makes less sense, it takes training data in form of 
% numeric matrices instead. 
%% Load the data
% Run the code below to load the data into the feature matrix |X| and response 
% vector |y|. We then also create the polynomial feature matrix, |Xpoly|. 

clear;
X = load('ex2data2.txt');
y = X(:,3);
X(:,3) = [];
% Create the polynomial feature matrix up to power 6
powers = [nchoosek(0:6,2); fliplr(nchoosek(0:6,2));1 1;2 2;3 3]';
powers(:,sum(powers)>6) = [];
Xpoly = (X(:,1).^powers(1,:)).*(X(:,2).^powers(2,:));
%% Fit the model
% Next, we train the model using |fitclinear| with the regularization type set 
% to |ridge| (this is the type of regularization used in ex2) and the strength 
% given by |lambda|. The result is a <https://www.mathworks.com/help/stats/classificationlinear-class.html 
% |ClassificationLinear|> model variable which contains all of the information 
% about the model. The model coefficients are found in the |Bias| and |Beta| properties 
% of the model variable. Use the control select a value of $\lambda$, then examine 
% the effect on the training accuracy and decision boundary from the results in 
% the next two sections:

% Choose lambda and train the model
lambda = 0.001;
logMdl = fitclinear(Xpoly,y,'Lambda',lambda,'Learner','logistic','Regularization','ridge')
logMdl.Bias
logMdl.Beta
%% Predict classes using the regularized model and plot the decision boundary
% The <https://www.mathworks.com/help/stats/classificationlinear.predict.html 
% |predict|> function is used below to classify data using the |ClassificationLinear| 
% model variable in the same manner as with |GeneralizedLinearModel| variables. 
% However, the |predict| function will return a _class label_ instead of probability 
% score. If needed, we obtain the probaibility scores by requesting a second output 
% from |predict|. See the code below used to plot the decision boundary. When 
% you are done, try re-training the model using a different value of |lambda| 
% and examine the effects. Which model do you think will generalize the best?

% Obtain the class labels and compute the training accuracy
Pass = predict(logMdl,Xpoly);
fprintf('Train Accuracy: %f\n', mean(Pass == y) * 100);
% Plot the positve and negative examples
figure; hold on;
plotMdlData(array2table([X y],'VariableNames',{'Test1','Test2','Pass'})); 

% Plot the decision boundary
xvals = linspace(min(X(:,1)), max(X(:,1)));
yvals = linspace(min(X(:,2)), max(X(:,2)));
[Xgrid, Ygrid] = meshgrid(xvals,yvals);
Xpolygrid = (Xgrid(:).^powers(1,:)).*(Ygrid(:).^powers(2,:));
[~,Score] = predict(logMdl,Xpolygrid); % Obtain the probability scores
contour(Xgrid,Ygrid,reshape(Score(:,2),size(Xgrid)),[0.5,0.5]); hold off;
% Labels and legend
xlabel('Test 1 score')
ylabel('Test 2 score')
legend('Pass', 'Fail','Decision Boundary')
%% Local Functions:
%% |*plotMdlData*|
% |plotMdlData| is used to plot the positive and negative examples for the data 
% sets for better comparison with the plots in ex2.

function [] = plotMdlData(data)
% Reproduce the plots from ex2 with positive and negative results for an input table
% Extract variable names from 3 column table
varNames = data.Properties.VariableNames;
% Plot the data with + for true and 0 for false examples
inds = data.(varNames{3}) == 1;
plot(data.(varNames{1})(inds), data.(varNames{2})(inds), 'k+','LineWidth', 2, 'MarkerSize', 7); 
inds = data.(varNames{3}) == 0;
plot(data.(varNames{1})(inds), data.(varNames{2})(inds), 'ko', 'MarkerFaceColor', 'y','MarkerSize', 7);
end
##### SOURCE END #####
--></body></html>